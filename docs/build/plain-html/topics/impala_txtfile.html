<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

<meta name="copyright" content="(C) Copyright 2024" />
<meta name="DC.rights.owner" content="(C) Copyright 2024" />
<meta name="DC.Type" content="concept" />
<meta name="DC.Title" content="Using Text Data Files with Impala Tables" />
<meta name="DC.Relation" scheme="URI" content="../topics/impala_file_formats.html" />
<meta name="prodname" content="Impala" />
<meta name="prodname" content="Impala" />
<meta name="prodname" content="Impala" />
<meta name="prodname" content="Impala" />
<meta name="prodname" content="Impala" />
<meta name="prodname" content="Impala" />
<meta name="prodname" content="Impala" />
<meta name="prodname" content="Impala" />
<meta name="version" content="Impala 3.4.x" />
<meta name="version" content="Impala 3.4.x" />
<meta name="version" content="Impala 3.4.x" />
<meta name="version" content="Impala 3.4.x" />
<meta name="version" content="Impala 3.4.x" />
<meta name="version" content="Impala 3.4.x" />
<meta name="version" content="Impala 3.4.x" />
<meta name="version" content="Impala 3.4.x" />
<meta name="DC.Format" content="XHTML" />
<meta name="DC.Identifier" content="txtfile" />
<link rel="stylesheet" type="text/css" href="../commonltr.css" />
<title>Using Text Data Files with Impala Tables</title>
</head>
<body id="txtfile">


  <h1 class="title topictitle1" id="ariaid-title1">Using Text Data Files with Impala Tables</h1>

  
  

  <div class="body conbody">

    <p class="p"> Impala supports using text files as the storage format for input and
      output. Text files are a convenient format to use for interchange with
      other applications or scripts that produce or read delimited text files,
      such as CSV or TSV with commas or tabs for delimiters. </p>


    <p class="p">
      Text files are also very flexible in their column definitions. For example, a text file could have more
      fields than the Impala table, and those extra fields are ignored during queries; or it could have fewer
      fields than the Impala table, and those missing fields are treated as <code class="ph codeph">NULL</code> values in
      queries. You could have fields that were treated as numbers or timestamps in a table, then use <code class="ph codeph">ALTER
      TABLE ... REPLACE COLUMNS</code> to switch them to strings, or the reverse.
    </p>


    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" class="table" frame="border" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 1. </span>Text Format Support in Impala</span></caption><colgroup><col style="width:10%" /><col style="width:10%" /><col style="width:20%" /><col style="width:30%" /><col style="width:30%" /></colgroup><thead class="thead" style="text-align:left;">
          <tr class="row">
            <th class="entry nocellnorowborder" style="vertical-align:top;" id="d226563e149">
              File Type
            </th>

            <th class="entry nocellnorowborder" style="vertical-align:top;" id="d226563e152">
              Format
            </th>

            <th class="entry nocellnorowborder" style="vertical-align:top;" id="d226563e155">
              Compression Codecs
            </th>

            <th class="entry nocellnorowborder" style="vertical-align:top;" id="d226563e158">
              Impala Can CREATE?
            </th>

            <th class="entry cell-norowborder" style="vertical-align:top;" id="d226563e161">
              Impala Can INSERT?
            </th>

          </tr>

        </thead>
<tbody class="tbody">
          <tr class="row">
            <td class="entry row-nocellborder" style="vertical-align:top;" headers="d226563e149 ">
              <a class="xref" href="impala_txtfile.html#txtfile">Text</a>
            </td>

            <td class="entry row-nocellborder" style="vertical-align:top;" headers="d226563e152 ">
              Unstructured
            </td>

            <td class="entry row-nocellborder" style="vertical-align:top;" headers="d226563e155 ">bzip2, deflate, gzip, LZO, Snappy, zstd</td>

            <td class="entry row-nocellborder" style="vertical-align:top;" headers="d226563e158 ">
              Yes. For <code class="ph codeph">CREATE TABLE</code> with no <code class="ph codeph">STORED AS</code> clause,
              the default file format is uncompressed text, with values separated by ASCII
              <code class="ph codeph">0x01</code> characters (typically represented as Ctrl-A).
            </td>

            <td class="entry cellrowborder" style="vertical-align:top;" headers="d226563e161 "> Yes if uncompressed.<p class="p">No if compressed.</p>
<p class="p">If LZO
                compression is used, you must create the table and load data in
                Hive.</p>
<p class="p">If other kinds of compression are used, you must
                load data through <code class="ph codeph">LOAD DATA</code>, Hive, or manually
                in HDFS. </p>
</td>

          </tr>

        </tbody>
</table>
</div>


    <p class="p toc inpage"></p>


  </div>


  <div class="related-links">
<div class="familylinks">
<div class="parentlink"><strong>Parent topic:</strong> <a class="link" href="../topics/impala_file_formats.html">How Impala Works with Hadoop File Formats</a></div>
</div>
</div><div class="topic concept nested1" aria-labelledby="ariaid-title2" id="text_performance">

    <h2 class="title topictitle2" id="ariaid-title2">Query Performance for Impala Text Tables</h2>

  

    <div class="body conbody">

      <p class="p">
        Data stored in text format is relatively bulky, and not as efficient to query as binary formats such as
        Parquet. You typically use text tables with Impala if that is the format you receive the data and you do
        not have control over that process, or if you are a relatively new Hadoop user and not familiar with
        techniques to generate files in other formats. (Because the default format for <code class="ph codeph">CREATE
        TABLE</code> is text, you might create your first Impala tables as text without giving performance much
        thought.) Either way, look for opportunities to use more efficient file formats for the tables used in your
        most performance-critical queries.
      </p>


      <p class="p">
        For frequently queried data, you might load the original text data files into one Impala table, then use an
        <code class="ph codeph">INSERT</code> statement to transfer the data to another table that uses the Parquet file format;
        the data is converted automatically as it is stored in the destination table.
      </p>


      <p class="p">
        For more compact data, consider using LZO compression for the text files. LZO is the only compression codec
        that Impala supports for text data, because the <span class="q">"splittable"</span> nature of LZO data files lets different
        nodes work on different parts of the same file in parallel. See <a class="xref" href="impala_txtfile.html#lzo">Using LZO-Compressed Text Files</a> for
        details.
      </p>


      <p class="p">
        You can also use text data compressed in the bzip2, deflate, gzip, Snappy, or
        zstd formats. Because these compressed formats are not <span class="q">"splittable"</span> in the way that LZO
        is, there is less opportunity for Impala to parallelize queries on them. Therefore, use
        these types of compressed data only for convenience if that is the format in which you
        receive the data. Prefer to use LZO compression for text data if you have the choice, or
        convert the data to Parquet using an <code class="ph codeph">INSERT ... SELECT</code> statement to copy
        the original data into a Parquet table. </p>


      <div class="note note"><span class="notetitle">Note:</span> 
        <p class="p">
          Impala supports bzip files created by the <code class="ph codeph">bzip2</code> command, but not bzip files with
          multiple streams created by the <code class="ph codeph">pbzip2</code> command. Impala decodes only the data from the
          first part of such files, leading to incomplete results.
        </p>

      </div>


        <p class="p">
          The maximum size that Impala can accommodate for an individual bzip file is 1 GB (after uncompression).
        </p>

        <p class="p">
          Impala supports zstd files created by the zstd command line tool.
        </p>


      <p class="p">
        In <span class="keyword">Impala 2.6</span> and higher, Impala queries are optimized for files
        stored in Amazon S3. For Impala tables that use the file formats Parquet, ORC, RCFile,
        SequenceFile, Avro, and uncompressed text, the setting
        <code class="ph codeph">fs.s3a.block.size</code> in the <span class="ph filepath">core-site.xml</span>
        configuration file determines how Impala divides the I/O work of reading the data files.
        This configuration setting is specified in bytes. By default, this value is 33554432 (32
        MB), meaning that Impala parallelizes S3 read operations on the files as if they were
        made up of 32 MB blocks. For example, if your S3 queries primarily access Parquet files
        written by MapReduce or Hive, increase <code class="ph codeph">fs.s3a.block.size</code> to 134217728
        (128 MB) to match the row group size of those files. If most S3 queries involve Parquet
        files written by Impala, increase <code class="ph codeph">fs.s3a.block.size</code> to 268435456 (256
        MB) to match the row group size produced by Impala.
      </p>


    </div>


  </div>


  <div class="topic concept nested1" aria-labelledby="ariaid-title3" id="text_ddl">

    <h2 class="title topictitle2" id="ariaid-title3">Creating Text Tables</h2>


    <div class="body conbody">

      <p class="p">
        <strong class="ph b">To create a table using text data files:</strong>
      </p>


      <p class="p">
        If the exact format of the text data files (such as the delimiter character) is not significant, use the
        <code class="ph codeph">CREATE TABLE</code> statement with no extra clauses at the end to create a text-format table. For
        example:
      </p>


<pre class="pre codeblock"><code>create table my_table(id int, s string, n int, t timestamp, b boolean);
</code></pre>

      <p class="p">
        The data files created by any <code class="ph codeph">INSERT</code> statements will use the Ctrl-A character (hex 01) as
        a separator between each column value.
      </p>


      <p class="p">
        A common use case is to import existing text files into an Impala table. The syntax is more verbose; the
        significant part is the <code class="ph codeph">FIELDS TERMINATED BY</code> clause, which must be preceded by the
        <code class="ph codeph">ROW FORMAT DELIMITED</code> clause. The statement can end with a <code class="ph codeph">STORED AS
        TEXTFILE</code> clause, but that clause is optional because text format tables are the default. For
        example:
      </p>


<pre class="pre codeblock"><code>create table csv(id int, s string, n int, t timestamp, b boolean)
  row format delimited
  <span class="ph" id="text_ddl__csv">fields terminated by ',';</span>

create table tsv(id int, s string, n int, t timestamp, b boolean)
  row format delimited
  <span class="ph" id="text_ddl__tsv">fields terminated by '\t';</span>

create table pipe_separated(id int, s string, n int, t timestamp, b boolean)
  row format delimited
  <span class="ph" id="text_ddl__psv">fields terminated by '|'</span>
  stored as textfile;
</code></pre>

      <p class="p">
        You can create tables with specific separator characters to import text files in familiar formats such as
        CSV, TSV, or pipe-separated. You can also use these tables to produce output data files, by copying data
        into them through the <code class="ph codeph">INSERT ... SELECT</code> syntax and then extracting the data files from the
        Impala data directory.
      </p>


      <p class="p">
        In Impala 1.3.1 and higher, you can specify a delimiter character <code class="ph codeph">'\</code><code class="ph codeph">0'</code> to
        use the ASCII 0 (<code class="ph codeph">nul</code>) character for text tables:
      </p>


<pre class="pre codeblock"><code>create table nul_separated(id int, s string, n int, t timestamp, b boolean)
  row format delimited
  fields terminated by '\0'
  stored as textfile;
</code></pre>

      <div class="note note"><span class="notetitle">Note:</span> 
        <p class="p">
          Do not surround string values with quotation marks in text data files that you construct. If you need to
          include the separator character inside a field value, for example to put a string value with a comma
          inside a CSV-format data file, specify an escape character on the <code class="ph codeph">CREATE TABLE</code> statement
          with the <code class="ph codeph">ESCAPED BY</code> clause, and insert that character immediately before any separator
          characters that need escaping.
        </p>

      </div>


      <p class="p">
        Issue a <code class="ph codeph">DESCRIBE FORMATTED <var class="keyword varname">table_name</var></code> statement to see the details of
        how each table is represented internally in Impala.
      </p>


      <p class="p">
        <strong class="ph b">Complex type considerations:</strong> Although you can create tables in this file format
        using the complex types (<code class="ph codeph">ARRAY</code>, <code class="ph codeph">STRUCT</code>, and
        <code class="ph codeph">MAP</code>) available in <span class="keyword">Impala 2.3</span> and higher,
        currently, Impala can query these types only in Parquet tables. <span class="ph">
        The one exception to the preceding rule is <code class="ph codeph">COUNT(*)</code> queries on RCFile
        tables that include complex types. Such queries are allowed in
        <span class="keyword">Impala 2.6</span> and higher. </span>
      </p>


    </div>


  </div>


  <div class="topic concept nested1" aria-labelledby="ariaid-title4" id="text_data_files">

    <h2 class="title topictitle2" id="ariaid-title4">Data Files for Text Tables</h2>


    <div class="body conbody">

      <p class="p">
        When Impala queries a table with data in text format, it consults all the data files in the data directory
        for that table, with some exceptions:
      </p>


      <ul class="ul">
        <li class="li">
          <p class="p">
            Impala ignores any hidden files, that is, files whose names start with a dot or an underscore.
          </p>

        </li>


        <li class="li">
          <p class="p">
        Impala queries ignore files with extensions commonly used for temporary work files by
        Hadoop tools. Any files with extensions <code class="ph codeph">.tmp</code> or
        <code class="ph codeph">.copying</code> are not considered part of the Impala table. The suffix
        matching is case-insensitive, so for example Impala ignores both
        <code class="ph codeph">.copying</code> and <code class="ph codeph">.COPYING</code> suffixes.
      </p>

        </li>


        <li class="li">
          <p class="p"> Impala uses suffixes to recognize when text data files are compressed text. For Impala
            to recognize the compressed text files, they must have the appropriate file extension
            corresponding to the compression codec, either <code class="ph codeph">.bz2</code>, <code class="ph codeph">.deflate</code>
            <code class="ph codeph">.gz</code>, <code class="ph codeph">.snappy</code>, or <code class="ph codeph">.zst</code>.
            The extensions can be in uppercase or lowercase. </p>

        </li>


        <li class="li"> Otherwise, the file names are not significant. When you put files
          into an HDFS directory through ETL jobs, or point Impala to an
          existing HDFS directory with the <code class="ph codeph">CREATE EXTERNAL
            TABLE</code> statement, or move data files under external control
          with the <code class="ph codeph">LOAD DATA</code> statement, Impala preserves the
          original file names. </li>

      </ul>


      <p class="p"> File names for data produced through Impala <code class="ph codeph">INSERT</code>
        statements are given unique names to avoid file name conflicts. </p>


      <p class="p">
        An <code class="ph codeph">INSERT ... SELECT</code> statement produces one data file from each node that processes the
        <code class="ph codeph">SELECT</code> part of the statement. An <code class="ph codeph">INSERT ... VALUES</code> statement produces a
        separate data file for each statement; because Impala is more efficient querying a small number of huge
        files than a large number of tiny files, the <code class="ph codeph">INSERT ... VALUES</code> syntax is not recommended
        for loading a substantial volume of data. If you find yourself with a table that is inefficient due to too
        many small data files, reorganize the data into a few large files by doing <code class="ph codeph">INSERT ...
        SELECT</code> to transfer the data to a new table.
      </p>


      <p class="p">
        <strong class="ph b">Special values within text data files:</strong>
      </p>


      <ul class="ul">
        <li class="li">
          <p class="p">
            Impala recognizes the literal strings <code class="ph codeph">inf</code> for infinity and <code class="ph codeph">nan</code> for
            <span class="q">"Not a Number"</span>, for <code class="ph codeph">FLOAT</code> and <code class="ph codeph">DOUBLE</code> columns.
          </p>

        </li>


        <li class="li">
          <div class="p"> Impala recognizes the literal string <code class="ph codeph">\N</code> to
            represent <code class="ph codeph">NULL</code>. When using Sqoop, specify the
            options <code class="ph codeph">--null-non-string</code> and
              <code class="ph codeph">--null-string</code> to ensure all <code class="ph codeph">NULL</code>
            values are represented correctly in the Sqoop output files.
              <code class="ph codeph">\N</code> needs to be escaped as in the below example:
            <pre class="pre codeblock"><code>--null-string '\\N' --null-non-string '\\N'</code></pre>
          </div>

        </li>

        <li class="li">
          <p class="p">By default, Sqoop writes <code class="ph codeph">NULL</code> values using the
            string <code class="ph codeph">null</code>, which causes a conversion error when
            such rows are evaluated by Impala. A workaround for existing tables
            and data files is to change the table properties through
              <code class="ph codeph">ALTER TABLE <var class="keyword varname">name</var> SET
              TBLPROPERTIES("serialization.null.format"="null")</code>.</p>

        </li>


        <li class="li">
          <div class="p">
        In <span class="keyword">Impala 2.6</span> and higher, Impala can optionally skip an arbitrary
        number of header lines from text input files on HDFS based on the
        <code class="ph codeph">skip.header.line.count</code> value in the <code class="ph codeph">TBLPROPERTIES</code>
        field of the table metadata. For example:
<pre class="pre codeblock"><code>create table header_line(first_name string, age int)
  row format delimited fields terminated by ',';

-- Back in the shell, load data into the table with commands such as:
-- cat &gt;data.csv
-- Name,Age
-- Alice,25
-- Bob,19
-- hdfs dfs -put data.csv /user/hive/warehouse/header_line

refresh header_line;

-- Initially, the Name,Age header line is treated as a row of the table.
select * from header_line limit 10;
+------------+------+
| first_name | age  |
+------------+------+
| Name       | NULL |
| Alice      | 25   |
| Bob        | 19   |
+------------+------+

alter table header_line set tblproperties('skip.header.line.count'='1');

-- Once the table property is set, queries skip the specified number of lines
-- at the beginning of each text data file. Therefore, all the files in the table
-- should follow the same convention for header lines.
select * from header_line limit 10;
+------------+-----+
| first_name | age |
+------------+-----+
| Alice      | 25  |
| Bob        | 19  |
+------------+-----+
</code></pre>
      </div>

        </li>

      </ul>


    </div>


  </div>


  <div class="topic concept nested1" aria-labelledby="ariaid-title5" id="text_etl">

    <h2 class="title topictitle2" id="ariaid-title5">Loading Data into Impala Text Tables</h2>

  

    <div class="body conbody">

      <p class="p">
        To load an existing text file into an Impala text table, use the <code class="ph codeph">LOAD DATA</code> statement and
        specify the path of the file in HDFS. That file is moved into the appropriate Impala data directory.
      </p>


      <p class="p">
        To load multiple existing text files into an Impala text table, use the <code class="ph codeph">LOAD DATA</code>
        statement and specify the HDFS path of the directory containing the files. All non-hidden files are moved
        into the appropriate Impala data directory.
      </p>


      <p class="p">
        To convert data to text from any other file format supported by Impala, use a SQL statement such as:
      </p>


<pre class="pre codeblock"><code>-- Text table with default delimiter, the hex 01 character.
CREATE TABLE text_table AS SELECT * FROM other_file_format_table;

-- Text table with user-specified delimiter. Currently, you cannot specify
-- the delimiter as part of CREATE TABLE LIKE or CREATE TABLE AS SELECT.
-- But you can change an existing text table to have a different delimiter.
CREATE TABLE csv LIKE other_file_format_table;
ALTER TABLE csv SET SERDEPROPERTIES ('serialization.format'=',', 'field.delim'=',');
INSERT INTO csv SELECT * FROM other_file_format_table;</code></pre>

      <p class="p">
        This can be a useful technique to see how Impala represents special values within a text-format data file.
        Use the <code class="ph codeph">DESCRIBE FORMATTED</code> statement to see the HDFS directory where the data files are
        stored, then use Linux commands such as <code class="ph codeph">hdfs dfs -ls <var class="keyword varname">hdfs_directory</var></code> and
        <code class="ph codeph">hdfs dfs -cat <var class="keyword varname">hdfs_file</var></code> to display the contents of an Impala-created
        text file.
      </p>


      <p class="p">
        To create a few rows in a text table for test purposes, you can use the <code class="ph codeph">INSERT ... VALUES</code>
        syntax:
      </p>


<pre class="pre codeblock"><code>INSERT INTO <var class="keyword varname">text_table</var> VALUES ('string_literal',100,hex('hello world'));</code></pre>

      <div class="note note"><span class="notetitle">Note:</span> 
        Because Impala and the HDFS infrastructure are optimized for multi-megabyte files, avoid the <code class="ph codeph">INSERT
        ... VALUES</code> notation when you are inserting many rows. Each <code class="ph codeph">INSERT ... VALUES</code>
        statement produces a new tiny file, leading to fragmentation and reduced performance. When creating any
        substantial volume of new data, use one of the bulk loading techniques such as <code class="ph codeph">LOAD DATA</code>
        or <code class="ph codeph">INSERT ... SELECT</code>. Or, <a class="xref" href="impala_hbase.html#impala_hbase">use an HBase
        table</a> for single-row <code class="ph codeph">INSERT</code> operations, because HBase tables are not subject to the
        same fragmentation issues as tables stored on HDFS.
      </div>


      <p class="p"> When you create a text file for use with an Impala text table, specify
          <code class="ph codeph">\N</code> to represent a <code class="ph codeph">NULL</code> value. For
        the differences between <code class="ph codeph">NULL</code> and empty strings, see
          <a class="xref" href="impala_literals.html#null">NULL</a>. </p>


      <p class="p">
        If a text file has fewer fields than the columns in the corresponding Impala table, all the corresponding
        columns are set to <code class="ph codeph">NULL</code> when the data in that file is read by an Impala query.
      </p>


      <p class="p">
        If a text file has more fields than the columns in the corresponding Impala table, the extra fields are
        ignored when the data in that file is read by an Impala query.
      </p>


      <p class="p">
        You can also use manual HDFS operations such as <code class="ph codeph">hdfs dfs -put</code> or <code class="ph codeph">hdfs dfs
        -cp</code> to put data files in the data directory for an Impala table. When you copy or move new data
        files into the HDFS directory for the Impala table, issue a <code class="ph codeph">REFRESH
        <var class="keyword varname">table_name</var></code> statement in <span class="keyword cmdname">impala-shell</span> before issuing the next
        query against that table, to make Impala recognize the newly added files.
      </p>


    </div>


  </div>


  <div class="topic concept nested1" aria-labelledby="ariaid-title6" id="lzo">

    <h2 class="title topictitle2" id="ariaid-title6">Using LZO-Compressed Text Files</h2>

  

    <div class="body conbody">

      <p class="p"> Impala supports using text data files that employ LZO compression.
        Where practical, apply compression to text data files. Impala queries
        are usually I/O-bound; reducing the amount of data read from disk
        typically speeds up a query, despite the extra CPU work to uncompress
        the data in memory. </p>


      <p class="p">
        Impala can work with LZO-compressed text files are preferable to files compressed by other codecs, because
        LZO-compressed files are <span class="q">"splittable"</span>, meaning that different portions of a file can be uncompressed
        and processed independently by different nodes.
      </p>


      <p class="p">
        Impala does not currently support writing LZO-compressed text files.
      </p>


      <p class="p">
        Because Impala can query LZO-compressed files but currently cannot write them, you use Hive to do the
        initial <code class="ph codeph">CREATE TABLE</code> and load the data, then switch back to Impala to run queries. For
        instructions on setting up LZO compression for Hive <code class="ph codeph">CREATE TABLE</code> and
        <code class="ph codeph">INSERT</code> statements, see
        <a class="xref" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LZO" target="_blank">the
        LZO page on the Hive wiki</a>. Once you have created an LZO text table, you can also manually add
        LZO-compressed text files to it, produced by the
        <a class="xref" href="http://www.lzop.org/" target="_blank"> <span class="keyword cmdname">lzop</span></a> command
        or similar method.
      </p>


      <div class="section" id="lzo__lzo_setup"><h3 class="title sectiontitle">Preparing to Use LZO-Compressed Text Files</h3>

        

        <p class="p">
          Before using LZO-compressed tables in Impala, do the following one-time setup for each machine in the
          cluster. Install the necessary packages using either the public repository, a private repository
          you establish, or by using packages. You must do these steps manually, whether or not you
          are using cluster management software.
        </p>


        <ol class="ol">
          <li class="li">
            <strong class="ph b">Prepare your systems to work with LZO by downloading and installing the appropriate libraries:</strong>

            <p class="p">
              Download and install the appropriate file to each machine on which you intend to use LZO with Impala.
            </p>

          </li>


          <li class="li">
            <strong class="ph b">Configure Impala to use LZO:</strong>
            <p class="p">
              Use <strong class="ph b">one</strong> of the following sets of commands to refresh your package management system's
              repository information, install the base LZO support for Hadoop, and install the LZO support for
              Impala.
            </p>


            <div class="note note"><span class="notetitle">Note:</span> 
              <p class="p">
                The name of the Hadoop LZO package changed in the distant past.
                Currently, the package name is <code class="ph codeph">hadoop-lzo</code>.
              </p>

            </div>


            <p class="p">
              <strong class="ph b">For RHEL/CentOS systems:</strong>
            </p>

<pre class="pre codeblock"><code>$ sudo yum update
$ sudo yum install hadoop-lzo
$ sudo yum install impala-lzo</code></pre>
            <p class="p">
              <strong class="ph b">For SUSE systems:</strong>
            </p>

<pre class="pre codeblock"><code>$ sudo apt-get update
$ sudo zypper install hadoop-lzo
$ sudo zypper install impala-lzo</code></pre>
            <p class="p">
              <strong class="ph b">For Debian/Ubuntu systems:</strong>
            </p>

<pre class="pre codeblock"><code>$ sudo zypper update
$ sudo apt-get install hadoop-lzo
$ sudo apt-get install impala-lzo</code></pre>
            <div class="note note"><span class="notetitle">Note:</span> 
              <p class="p">
                The level of the <code class="ph codeph">impala-lzo</code> package is closely tied to the version of Impala
                you use. Any time you upgrade Impala, re-do the installation command for
                <code class="ph codeph">impala-lzo</code> on each applicable machine to make sure you have the appropriate
                version of that package.
              </p>

            </div>

          </li>


          <li class="li">
            For <code class="ph codeph">core-site.xml</code> on the client <strong class="ph b">and</strong> server (that is, in the configuration
            directories for both Impala and Hadoop), append <code class="ph codeph">com.hadoop.compression.lzo.LzopCodec</code>
            to the comma-separated list of codecs. For example:
<pre class="pre codeblock"><code>&lt;property&gt;
  &lt;name&gt;io.compression.codecs&lt;/name&gt;
  &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,
        org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,
        org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzopCodec&lt;/value&gt;
&lt;/property&gt;</code></pre>
            <div class="note note"><span class="notetitle">Note:</span> 
              <p class="p">
                If this is the first time you have edited the Hadoop <span class="ph filepath">core-site.xml</span> file, note
                that the <span class="ph filepath">/etc/hadoop/conf</span> directory is typically a symbolic link, so the
                canonical <span class="ph filepath">core-site.xml</span> might reside in a different directory:
              </p>

<pre class="pre codeblock"><code>$ ls -l /etc/hadoop
total 8
lrwxrwxrwx. 1 root root   29 Feb 26  2013 conf -&gt; /etc/alternatives/hadoop-conf
lrwxrwxrwx. 1 root root   10 Feb 26  2013 conf.dist -&gt; conf.empty
drwxr-xr-x. 2 root root 4096 Feb 26  2013 conf.empty
drwxr-xr-x. 2 root root 4096 Oct 28 15:46 conf.pseudo</code></pre>
              <p class="p">
                If the <code class="ph codeph">io.compression.codecs</code> property is missing from
                <span class="ph filepath">core-site.xml</span>, only add <code class="ph codeph">com.hadoop.compression.lzo.LzopCodec</code>
                to the new property value, not all the names from the preceding example.
              </p>

            </div>

          </li>


          <li class="li">
            Restart the MapReduce and Impala services.
          </li>

        </ol>


      </div>


      <div class="section" id="lzo__lzo_create_table"><h3 class="title sectiontitle">Creating LZO Compressed Text Tables</h3>

        

        <p class="p">
          A table containing LZO-compressed text files must be created in Hive with the following storage clause:
        </p>


<pre class="pre codeblock"><code>STORED AS
    INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
    OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'</code></pre>

        <p class="p">
          Also, certain Hive settings need to be in effect. For example:
        </p>


<pre class="pre codeblock"><code>hive&gt; SET mapreduce.output.fileoutputformat.compress=true;
hive&gt; SET hive.exec.compress.output=true;
hive&gt; SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzopCodec;
hive&gt; CREATE TABLE lzo_t (s string) STORED AS
  &gt; INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  &gt; OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat';
hive&gt; INSERT INTO TABLE lzo_t SELECT col1, col2 FROM uncompressed_text_table;</code></pre>

        <p class="p">
          Once you have created LZO-compressed text tables, you can convert data stored in other tables (regardless
          of file format) by using the <code class="ph codeph">INSERT ... SELECT</code> statement in Hive.
        </p>


        <p class="p">
          Files in an LZO-compressed table must use the <code class="ph codeph">.lzo</code> extension. Examine the files in the
          HDFS data directory after doing the <code class="ph codeph">INSERT</code> in Hive, to make sure the files have the
          right extension. If the required settings are not in place, you end up with regular uncompressed files,
          and Impala cannot access the table because it finds data files with the wrong (uncompressed) format.
        </p>


        <p class="p">
          After loading data into an LZO-compressed text table, index the files so that they can be split. You
          index the files by running a Java class,
          <code class="ph codeph">com.hadoop.compression.lzo.DistributedLzoIndexer</code>, through the Linux command line. This
          Java class is included in the <code class="ph codeph">hadoop-lzo</code> package.
        </p>


        <p class="p">
          Run the indexer using a command like the following:
        </p>


<pre class="pre codeblock"><code>$ hadoop jar /usr/lib/hadoop/lib/hadoop-lzo-<var class="keyword varname">version</var>-gplextras.jar
  com.hadoop.compression.lzo.DistributedLzoIndexer /<var class="keyword varname">hdfs_location_of_table</var>/</code></pre>

        <div class="note note"><span class="notetitle">Note:</span> 
          If the path of the JAR file in the preceding example is not recognized, do a <span class="keyword cmdname">find</span>
          command to locate <span class="ph filepath">hadoop-lzo-*-gplextras.jar</span> and use that path.
        </div>


        <p class="p">
          Indexed files have the same name as the file they index, with the <code class="ph codeph">.index</code> extension. If
          the data files are not indexed, Impala queries still work, but the queries read the data from remote
          DataNodes, which is very inefficient.
        </p>


        <p class="p">
          Once the LZO-compressed tables are created, and data is loaded and indexed, you can query them through
          Impala. As always, the first time you start <span class="keyword cmdname">impala-shell</span> after creating a table in
          Hive, issue an <code class="ph codeph">INVALIDATE METADATA</code> statement so that Impala recognizes the new table.
          (In Impala 1.2 and higher, you only have to run <code class="ph codeph">INVALIDATE METADATA</code> on one node, rather
          than on all the Impala nodes.)
        </p>


      </div>


    </div>


  </div>


  <div class="topic concept nested1" aria-labelledby="ariaid-title7" id="gzip">

    <h2 class="title topictitle2" id="ariaid-title7">Using bzip2, deflate, gzip, Snappy, or zstd Text Files</h2>

  

    <div class="body conbody">

      <p class="p"> Impala supports using text data files that employ bzip2, deflate, gzip, Snappy, or zstd
        compression. These compression types are primarily for convenience within an existing ETL
        pipeline rather than maximum performance. Although it requires less I/O to read compressed
        text than the equivalent uncompressed text, files compressed by these codecs are not
          <span class="q">"splittable"</span> and therefore cannot take full advantage of the Impala parallel query
        capability. Impala can read compressed text files written by Hive.</p>


      <p class="p"> As each Snappy-compressed file is processed, the node doing the work reads the entire file
        into memory and then decompresses it. Therefore, the node must have enough memory to hold
        both the compressed and uncompressed data from the text file. The memory required to hold
        the uncompressed data is difficult to estimate in advance, potentially causing problems on
        systems with low memory limits or with resource management enabled. <span class="ph">This
          memory overhead is reduced for bzip2-, deflate-, gzip-, and zstd-compressed text files. The
          compressed data is decompressed as it is read, rather than all at once.</span>
      </p>


      <p class="p"> To create a table to hold compressed text, create a text table with no special compression
        options. Specify the delimiter and escape character if required, using the <code class="ph codeph">ROW
          FORMAT</code> clause. </p>


      <p class="p">
        Because Impala can query compressed text files but currently cannot write them, produce the compressed text
        files outside Impala and use the <code class="ph codeph">LOAD DATA</code> statement, manual HDFS commands to move them to
        the appropriate Impala data directory. (Or, you can use <code class="ph codeph">CREATE EXTERNAL TABLE</code> and point
        the <code class="ph codeph">LOCATION</code> attribute at a directory containing existing compressed text files.)
      </p>


      <p class="p">
        The following example shows how you can create a regular text table, put different kinds of compressed and
        uncompressed files into it, and Impala automatically recognizes and decompresses each one based on their
        file extensions:
      </p>


<pre class="pre codeblock"><code>create table csv_compressed (a string, b string, c string)
  row format delimited fields terminated by ",";

insert into csv_compressed values
  ('one - uncompressed', 'two - uncompressed', 'three - uncompressed'),
  ('abc - uncompressed', 'xyz - uncompressed', '123 - uncompressed');
...make equivalent .bz2, .gz, .snappy, and .zst files and load them into same table directory...

select * from csv_compressed;
+--------------------+--------------------+----------------------+
| a                  | b                  | c                    |
+--------------------+--------------------+----------------------+
| one - snappy       | two - snappy       | three - snappy       |
| one - uncompressed | two - uncompressed | three - uncompressed |
| abc - uncompressed | xyz - uncompressed | 123 - uncompressed   |
| one - bz2          | two - bz2          | three - bz2          |
| abc - bz2          | xyz - bz2          | 123 - bz2            |
| one - gzip         | two - gzip         | three - gzip         |
| abc - gzip         | xyz - gzip         | 123 - gzip           |
| one - zstd         | two - zstd         | three - zstd         |
| abc - zstd         | xyz - zstd         | 123 - zstd           |
| one - deflate      | two - deflate      | three - deflate      |
| abc - deflate      | xyz - deflate      | 123 - deflate        |
+--------------------+--------------------+----------------------+

$ hdfs dfs -ls 'hdfs://127.0.0.1:8020/user/hive/warehouse/file_formats.db/csv_compressed/';
...truncated for readability...
75 hdfs://127.0.0.1:8020/user/hive/warehouse/file_formats.db/csv_compressed/csv_compressed.snappy
79 hdfs://127.0.0.1:8020/user/hive/warehouse/file_formats.db/csv_compressed/csv_compressed_bz2.csv.bz2
80 hdfs://127.0.0.1:8020/user/hive/warehouse/file_formats.db/csv_compressed/csv_compressed_gzip.csv.gz
58 hdfs://127.0.0.1:8020/user/hive/warehouse/file_formats.db/csv_compressed/csv_compressed_zstd.csv.zst
48 hdfs://127.0.0.1:8020/user/hive/warehouse/file_formats.db/csv_compressed/csv_compressed_deflate.csv.deflate
116 hdfs://127.0.0.1:8020/user/hive/warehouse/file_formats.db/csv_compressed/dd414df64d67d49b_data.0.
</code></pre>

    </div>


  </div>


</body>
</html>