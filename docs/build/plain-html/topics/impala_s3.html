<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

<meta name="copyright" content="(C) Copyright 2025" />
<meta name="DC.rights.owner" content="(C) Copyright 2025" />
<meta name="DC.Type" content="concept" />
<meta name="DC.Title" content="Using Impala with Amazon S3 Object Store" />
<meta name="prodname" content="Impala" />
<meta name="prodname" content="Impala" />
<meta name="version" content="Impala 3.4.x" />
<meta name="version" content="Impala 3.4.x" />
<meta name="DC.Format" content="XHTML" />
<meta name="DC.Identifier" content="s3" />
<link rel="stylesheet" type="text/css" href="../commonltr.css" />
<title>Using Impala with Amazon S3 Object Store</title>
</head>
<body id="s3">


  <h1 class="title topictitle1" id="ariaid-title1">Using Impala with Amazon S3 Object Store</h1>

  
  

  <div class="body conbody">

    <p class="p"> You can use Impala to query data residing on the Amazon S3
      object store. This capability allows convenient access to a storage system
      that is remotely managed, accessible from anywhere, and integrated with
      various cloud-based services. Impala can query files in any supported file
      format from S3. The S3 storage location can be for an entire table, or
      individual partitions in a partitioned table. </p>


    <p class="p toc inpage"></p>


  </div>

  <div class="topic concept nested1" aria-labelledby="ariaid-title2" id="s3_best_practices">
    <h2 class="title topictitle2" id="ariaid-title2">Best Practices for Using Impala with S3</h2>

    
    <div class="body conbody">
      <p class="p"> The following guidelines summarize the best practices described in the
        rest of this topic: </p>

      <ul class="ul">
        <li class="li">
          <p class="p"> Any reference to an S3 location must be fully qualified when S3 is
            not designated as the default storage, for example,
              <code class="ph codeph">s3a:://[s3-bucket-name]</code>.</p>

        </li>

        <li class="li">
          <p class="p"> Set <code class="ph codeph">fs.s3a.connection.maximum</code> to 1500 for
              <span class="keyword cmdname">impalad</span>. </p>

        </li>

        <li class="li">
          <p class="p"> Set <code class="ph codeph">fs.s3a.block.size</code> to 134217728 (128 MB in
            bytes) if most Parquet files queried by Impala were written by Hive
            or ParquetMR jobs. </p>

          <p class="p">Set the block size to 268435456 (256 MB in bytes) if most Parquet
            files queried by Impala were written by Impala. </p>

          <p class="p">Starting in Impala 3.4.0, instead of
              <code class="ph codeph">fs.s3a.block.size</code>, the
              <code class="ph codeph">PARQUET_OBJECT_STORE_SPLIT_SIZE</code> query option
            controls the Parquet-specific split size. The default value is 256
            MB.</p>

        </li>

        <li class="li">
          <p class="p">
            <code class="ph codeph">DROP TABLE .. PURGE</code> is much faster than the default
              <code class="ph codeph">DROP TABLE</code>. The same applies to <code class="ph codeph">ALTER
              TABLE ... DROP PARTITION PURGE</code> versus the default
              <code class="ph codeph">DROP PARTITION</code> operation. Due to the eventually
            consistent nature of S3, the files for that table or partition could
            remain for some unbounded time when using <code class="ph codeph">PURGE</code>.
            The default <code class="ph codeph">DROP TABLE/PARTITION</code> is slow because
            Impala copies the files to the S3A trash folder, and Impala waits
            until all the data is moved. <code class="ph codeph">DROP TABLE/PARTITION ..
              PURGE</code> is a fast delete operation, and the Impala
            statement finishes quickly even though the change might not have
            propagated fully throughout S3. </p>

        </li>

        <li class="li">
          <p class="p">
            <code class="ph codeph">INSERT</code> statements are faster than <code class="ph codeph">INSERT
              OVERWRITE</code> for S3. The query option
              <code class="ph codeph">S3_SKIP_INSERT_STAGING</code>, which is set to
              <code class="ph codeph">true</code> by default, skips the staging step for
            regular <code class="ph codeph">INSERT</code> (but not <code class="ph codeph">INSERT
              OVERWRITE</code>). This makes the operation much faster, but
            consistency is not guaranteed: if a node fails during execution, the
            table could end up with inconsistent data. Set this option to
              <code class="ph codeph">false</code> if stronger consistency is required,
            however, this setting will make the <code class="ph codeph">INSERT</code>
            operations slower. </p>

          <ul class="ul">
            <li class="li">
              <p class="p"> For Impala-ACID tables, both <code class="ph codeph">INSERT</code> and
                  <code class="ph codeph">INSERT OVERWRITE</code> tables for S3 are fast,
                regardless of the setting of
                  <code class="ph codeph">S3_SKIP_INSERT_STAGING</code>. Plus, consistency is
                guaranteed with ACID tables.</p>

            </li>

          </ul>

        </li>

        <li class="li">Enable <a class="xref" href="impala_data_cache.html#data_cache">data cache for
            remote reads</a>.</li>

        <li class="li">Enable <a class="xref" href="https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/s3guard.html" target="_blank">S3Guard</a> in your cluster for
          data consistency.</li>

        <li class="li">
          <p class="p"> Too many files in a table can make metadata load and update slow
            in S3. If too many requests are made to S3, S3 has a back-off
            mechanism and responds slower than usual.</p>

          <ul class="ul">
            <li class="li">If you have many small files due to over-granular partitioning,
              configure partitions with many megabytes of data so that even a
              query against a single partition can be parallelized effectively. </li>

            <li class="li">If you have many small files because of many small
                <code class="ph codeph">INSERT</code> queries, use bulk
                <code class="ph codeph">INSERT</code>s so that more data is written to fewer
              files. </li>

          </ul>

        </li>

      </ul>

    </div>

  </div>


  <div class="topic concept nested1" aria-labelledby="ariaid-title3" id="s3_sql">
    <h2 class="title topictitle2" id="ariaid-title3">How Impala SQL Statements Work with S3</h2>

    <div class="body conbody">
      <p class="p"> Impala SQL statements work with data in S3 as follows: </p>

      <ul class="ul">
        <li class="li">
          <p class="p"> The <a class="xref" href="impala_create_table.html#create_table">CREATE
              TABLE</a> or <a class="xref" href="impala_alter_table.html#alter_table">ALTER TABLE</a> statement can specify that a table resides in
            the S3 object store by encoding an <code class="ph codeph">s3a://</code> prefix
            for the <code class="ph codeph">LOCATION</code> property. <code class="ph codeph">ALTER
              TABLE</code> can also set the <code class="ph codeph">LOCATION</code> property
            for an individual partition so that some data in a table resides in
            S3 and other data in the same table resides on HDFS. </p>

        </li>

        <li class="li">
          <p class="p"> Once a table or partition is designated as residing in S3, the
              <a class="xref" href="impala_select.html#select">SELECT Statement</a> statement transparently
            accesses the data files from the appropriate storage layer. </p>

        </li>

        <li class="li">
          <p class="p">
            If the S3 table is an internal table, the <a class="xref" href="impala_drop_table.html#drop_table">DROP TABLE</a> statement
            removes the corresponding data files from S3 when the table is dropped.
          </p>

        </li>

        <li class="li">
          <p class="p"> The <a class="xref" href="impala_truncate_table.html#truncate_table">TRUNCATE
              TABLE</a> statement always removes the corresponding
            data files from S3 when the table is truncated. </p>

        </li>

        <li class="li">
          <p class="p">
            The <a class="xref" href="impala_load_data.html#load_data">LOAD DATA</a>
            statement can move data files residing in HDFS into
            an S3 table.
          </p>

        </li>

        <li class="li">
          <p class="p">
            The <a class="xref" href="impala_insert.html#insert">INSERT</a> statement, or the <code class="ph codeph">CREATE TABLE AS SELECT</code>
            form of the <code class="ph codeph">CREATE TABLE</code> statement, can copy data from an HDFS table or another S3
            table into an S3 table. The <a class="xref" href="impala_s3_skip_insert_staging.html#s3_skip_insert_staging">S3_SKIP_INSERT_STAGING</a>
            query option chooses whether or not to use a fast code path for these write operations to S3,
            with the tradeoff of potential inconsistency in the case of a failure during the statement.
          </p>

        </li>

      </ul>

      <p class="p">
        For usage information about Impala SQL statements with S3 tables, see <a class="xref" href="impala_s3.html#s3_ddl">Creating Impala Databases, Tables, and Partitions for Data Stored in S3</a>
        and <a class="xref" href="impala_s3.html#s3_dml">Using Impala DML Statements for S3 Data</a>.
      </p>

    </div>

  </div>


  <div class="topic concept nested1" aria-labelledby="ariaid-title4" id="s3_creds">

    <h2 class="title topictitle2" id="ariaid-title4">Specifying Impala Credentials to Access Data in S3</h2>


    <div class="body conbody">

      <p class="p"> To allow Impala to access data in S3, specify values for the following
        configuration settings in your <span class="ph filepath">core-site.xml</span> file: </p>

<pre class="pre codeblock"><code>
&lt;property&gt;
&lt;name&gt;fs.s3a.access.key&lt;/name&gt;
&lt;value&gt;<var class="keyword varname">your_access_key</var>&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;fs.s3a.secret.key&lt;/name&gt;
&lt;value&gt;<var class="keyword varname">your_secret_key</var>&lt;/value&gt;
&lt;/property&gt;
</code></pre>

      <p class="p"> After specifying the credentials, restart both the Impala and Hive
        services. Restarting Hive is required because Impala statements, such as
          <code class="ph codeph">CREATE TABLE</code>, go through the Hive Metastore. </p>


      <div class="note important"><span class="importanttitle">Important:</span> 
          <p class="p">
            Although you can specify the access key ID and secret key as part of the <code class="ph codeph">s3a://</code> URL in the
            <code class="ph codeph">LOCATION</code> attribute, doing so makes this sensitive information visible in many places, such
            as <code class="ph codeph">DESCRIBE FORMATTED</code> output and Impala log files. Therefore, specify this information
            centrally in the <span class="ph filepath">core-site.xml</span> file, and restrict read access to that file to only
            trusted users.
          </p>

      </div>

      <p class="p">See <a class="xref" href="https://www.google.com/url?q=https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html%23Authenticating_with_S3&amp;sa=D&amp;ust=1572980027740000&amp;usg=AFQjCNFnzPSfNBMVRgJZRenvhLblezHbdw" target="_blank">Authenticating with S3</a> for
        additional authentication mechanisms to access S3.</p>


    </div>


  </div>


  <div class="topic concept nested1" aria-labelledby="ariaid-title5" id="s3_etl">

    <h2 class="title topictitle2" id="ariaid-title5">Loading Data into S3 for Impala Queries</h2>

  

    <div class="body conbody">

      <p class="p">
        If your ETL pipeline involves moving data into S3 and then querying through Impala,
        you can either use Impala DML statements to create, move, or copy the data, or
        use the same data loading techniques as you would for non-Impala data.
      </p>


    </div>


    <div class="topic concept nested2" aria-labelledby="ariaid-title6" id="s3_dml">
      <h3 class="title topictitle3" id="ariaid-title6">Using Impala DML Statements for S3 Data</h3>

      <div class="body conbody">
        <p class="p">The Impala DML statements (<code class="ph codeph">INSERT</code>, <code class="ph codeph">LOAD
            DATA</code>, and <code class="ph codeph">CREATE TABLE AS SELECT</code>) can
          write data into a table or partition that resides in S3. The syntax of
          the DML statements is the same as for any other tables because the S3
          location for tables and partitions is specified by an
            <code class="ph codeph">s3a://</code> prefix in the <code class="ph codeph">LOCATION</code>
          attribute of <code class="ph codeph">CREATE TABLE</code> or <code class="ph codeph">ALTER
            TABLE</code> statements. If you bring data into S3 using the
          normal S3 transfer mechanisms instead of Impala DML statements, issue
          a <code class="ph codeph">REFRESH</code> statement for the table before using Impala
          to query the S3 data.</p>

        <p class="p"> Because of differences
        between S3 and traditional filesystems, DML operations for S3 tables can
        take longer than for tables on HDFS. For example, both the <code class="ph codeph">LOAD
          DATA</code> statement and the final stage of the
          <code class="ph codeph">INSERT</code> and <code class="ph codeph">CREATE TABLE AS SELECT</code>
        statements involve moving files from one directory to another. (In the
        case of <code class="ph codeph">INSERT</code> and <code class="ph codeph">CREATE TABLE AS
          SELECT</code>, the files are moved from a temporary staging
        directory to the final destination directory.) Because S3 does not
        support a <span class="q">"rename"</span> operation for existing objects, in these cases
        Impala actually copies the data files from one location to another and
        then removes the original files. In <span class="keyword">Impala 2.6</span>,
        the <code class="ph codeph">S3_SKIP_INSERT_STAGING</code> query option provides a way
        to speed up <code class="ph codeph">INSERT</code> statements for S3 tables and
        partitions, with the tradeoff that a problem during statement execution
        could leave data in an inconsistent state. It does not apply to
          <code class="ph codeph">INSERT OVERWRITE</code> or <code class="ph codeph">LOAD DATA</code>
        statements. See <a class="xref" href="../shared/../topics/impala_s3_skip_insert_staging.html#s3_skip_insert_staging">S3_SKIP_INSERT_STAGING Query Option</a> for details. </p>

      </div>

    </div>


    <div class="topic concept nested2" aria-labelledby="ariaid-title7" id="s3_manual_etl">
      <h3 class="title topictitle3" id="ariaid-title7">Manually Loading Data into Impala Tables in S3</h3>

      <div class="body conbody">
        <p class="p">
          As an alternative, or on earlier Impala releases without DML support for S3,
          you can use the Amazon-provided methods to bring data files into S3 for querying through Impala. See
          <a class="xref" href="http://aws.amazon.com/s3/" target="_blank">the Amazon S3 web site</a> for
          details.
        </p>


        <div class="note important"><span class="importanttitle">Important:</span> 
          <div class="p"> For best
        compatibility with the S3 write support in <span class="keyword">Impala 2.6</span> and higher: <ul class="ul">
          <li class="li"> Use native Hadoop techniques to create data files in S3 for
            querying through Impala. </li>

          <li class="li"> Use the <code class="ph codeph">PURGE</code> clause of <code class="ph codeph">DROP
              TABLE</code> when dropping internal (managed) tables. </li>

        </ul>
 By default, when you drop an internal (managed) table, the data
        files are moved to the HDFS trashcan. This operation is expensive for
        tables that reside on the Amazon S3 object store. Therefore, for S3
        tables, prefer to use <code class="ph codeph">DROP TABLE <var class="keyword varname">table_name</var>
          PURGE</code> rather than the default <code class="ph codeph">DROP TABLE</code>
        statement. The <code class="ph codeph">PURGE</code> clause makes Impala delete the
        data files immediately, skipping the HDFS trashcan. For the
          <code class="ph codeph">PURGE</code> clause to work effectively, you must originally
        create the data files on S3 using one of the tools from the Hadoop
        ecosystem, such as <code class="ph codeph">hadoop fs -cp</code>, or
          <code class="ph codeph">INSERT</code> in Impala or Hive. </div>

        </div>


        <p class="p"> After you upload data files to a location already mapped to an
          Impala table or partition, or if you delete files in S3 from such a
          location, issue the <code class="ph codeph">REFRESH</code> statement to make Impala
          aware of the new set of data files. </p>


      </div>

    </div>


  </div>


  <div class="topic concept nested1" aria-labelledby="ariaid-title8" id="s3_ddl">

    <h2 class="title topictitle2" id="ariaid-title8">Creating Impala Databases, Tables, and Partitions for Data Stored in
      S3</h2>

  

    <div class="body conbody">
      <p class="p">To create a table that resides in S3, run the <code class="ph codeph">CREATE
          TABLE</code> or <code class="ph codeph">ALTER TABLE</code> statement with the
          <code class="ph codeph">LOCATION</code> clause. </p>

      <p class="p"><code class="ph codeph">ALTER TABLE</code> can set the <code class="ph codeph">LOCATION</code>
        property for an individual partition, so that some data in a table
        resides in S3 and other data in the same table resides on HDFS.</p>

      <p class="p">The syntax for the <code class="ph codeph">LOCATION</code> clause is:</p>

      <pre class="pre codeblock"><code>LOCATION 's3a://<var class="keyword varname">bucket_name</var>/<var class="keyword varname">path</var>/<var class="keyword varname">to</var>/<var class="keyword varname">file</var>'</code></pre>
      <p class="p">The file system prefix is always <code class="ph codeph">s3a://</code>. Impala does
        not support the <code class="ph codeph">s3://</code> or <code class="ph codeph">s3n://</code>
        prefixes. </p>

      <p class="p"> For a partitioned table, either specify a separate
          <code class="ph codeph">LOCATION</code> clause for each new partition, or specify a
        base <code class="ph codeph">LOCATION</code> for the table and set up a directory
        structure in S3 to mirror the way Impala partitioned tables are
        structured in S3. </p>


      <p class="p"> You point a nonpartitioned table or an individual partition at S3 by
        specifying a single directory path in S3, which could be any arbitrary
        directory. To replicate the structure of an entire Impala partitioned
        table or database in S3 requires more care, with directories and
        subdirectories nested and named to match the equivalent directory tree
        in HDFS. Consider setting up an empty staging area if necessary in HDFS,
        and recording the complete directory structure so that you can replicate
        it in S3.  </p>

      <p class="p"> When working with multiple tables with data files stored in S3, you can
        create a database with a <code class="ph codeph">LOCATION</code> attribute pointing to
        an S3 path. Specify a URL of the form
            <code class="ph codeph">s3a://<var class="keyword varname">bucket</var>/<var class="keyword varname">root</var>/<var class="keyword varname">path</var>/<var class="keyword varname">for</var>/<var class="keyword varname">database</var></code>
        for the <code class="ph codeph">LOCATION</code> attribute of the database. Any tables
        created inside that database automatically create directories underneath
        the one specified by the database <code class="ph codeph">LOCATION</code> attribute. </p>

      <p class="p">The following example creates a table with one partition for the year
        2017 resides on HDFS and one partition for the year 2018 resides in
        S3.</p>


      <p class="p">The partition for year 2018 includes a <code class="ph codeph">LOCATION</code>
        attribute with an <code class="ph codeph">s3a://</code> URL, and so refers to data
        residing in S3, under a specific path underneath the bucket
          <code class="ph codeph">impala-demo</code>. </p>


<pre class="pre codeblock"><code>CREATE TABLE mostly_on_hdfs (x int) PARTITIONED BY (year INT);
ALTER TABLE mostly_on_hdfs ADD PARTITION (year=2017);
ALTER TABLE mostly_on_hdfs ADD PARTITION (year=2018) 
   LOCATION 's3a://impala-demo/dir1/dir2/dir3/t1';
</code></pre>

      <p class="p"> The following session creates a database and two partitioned tables
        residing entirely in S3, one partitioned by a single column and the
        other partitioned by multiple columns. </p>

      <ul class="ul">
        <li class="li">Because a <code class="ph codeph">LOCATION</code> attribute with an
            <code class="ph codeph">s3a://</code> URL is specified for the database, the
          tables inside that database are automatically created in S3 underneath
          the database directory. </li>

        <li class="li">To see the names of the associated subdirectories, including the
          partition key values, use an S3 client tool to examine how the
          directory structure is organized in S3. </li>

      </ul>


<pre class="pre codeblock"><code>CREATE DATABASE db_on_s3 LOCATION 's3a://impala-demo/dir1/dir2/dir3';
CREATE TABLE partitioned_multiple_keys (x INT)
   PARTITIONED BY (year SMALLINT, month TINYINT, day TINYINT);

ALTER TABLE partitioned_multiple_keys
   ADD PARTITION (year=2015,month=1,day=1);
ALTER TABLE partitioned_multiple_keys
   ADD PARTITION (year=2015,month=1,day=31);

!hdfs dfs -ls -R s3a://impala-demo/dir1/dir2/dir3
2015-03-17 13:56:34          0 dir1/dir2/dir3/
2015-03-17 16:47:13          0 dir1/dir2/dir3/partitioned_multiple_keys/
2015-03-17 16:47:44          0 dir1/dir2/dir3/partitioned_multiple_keys/year=2015/month=1/day=1/
2015-03-17 16:47:50          0 dir1/dir2/dir3/partitioned_multiple_keys/year=2015/month=1/day=31/</code></pre>

      <p class="p">
        The <code class="ph codeph">CREATE DATABASE</code> and <code class="ph codeph">CREATE TABLE</code> statements create the associated
        directory paths if they do not already exist. You can specify multiple levels of directories, and the
        <code class="ph codeph">CREATE</code> statement creates all appropriate levels, similar to using <code class="ph codeph">mkdir
        -p</code>.
      </p>


      <p class="p"> Use the standard S3 file upload methods to put the actual data files
        into the right locations. You can also put the directory paths and data
        files in place before creating the associated Impala databases or
        tables, and Impala automatically uses the data from the appropriate
        location after the associated databases and tables are created. </p>

      <p class="p">Use the <code class="ph codeph">ALTER TABLE</code> statement with the
          <code class="ph codeph">LOCATION</code> clause to switch whether an existing table
        or partition points to data in HDFS or S3. For example, if you have an
        Impala table or partition pointing to data files in HDFS or S3, and you
        later transfer those data files to the other filesystem, use the
          <code class="ph codeph">ALTER TABLE</code> statement to adjust the
          <code class="ph codeph">LOCATION</code> attribute of the corresponding table or
        partition to reflect that change. </p>


    </div>


  </div>


  <div class="topic concept nested1" aria-labelledby="ariaid-title9" id="s3_internal_external">

    <h2 class="title topictitle2" id="ariaid-title9">Internal and External Tables Located in S3</h2>


    <div class="body conbody">

      <p class="p"> Just as with tables located on HDFS storage, you can designate
        S3-based tables as either internal (managed by Impala) or external, by
        using the syntax <code class="ph codeph">CREATE TABLE</code> or <code class="ph codeph">CREATE
          EXTERNAL TABLE</code> respectively. </p>

      <p class="p">When you drop an internal table, the files associated with the table
        are removed, even if they are in S3 storage. When you drop an external
        table, the files associated with the table are left alone, and are still
        available for access by other tools or components.</p>


      <p class="p"> If the data in S3 is intended to be long-lived and accessed by other
        tools in addition to Impala, create any associated S3 tables with the
          <code class="ph codeph">CREATE EXTERNAL TABLE</code> syntax, so that the files are
        not deleted from S3 when the table is dropped. </p>


      <p class="p"> If the data in S3 is only needed for querying by Impala and can be
        safely discarded once the Impala workflow is complete, create the
        associated S3 tables using the <code class="ph codeph">CREATE TABLE</code> syntax, so
        that dropping the table also deletes the corresponding data files in S3. </p>


    </div>


  </div>


  <div class="topic concept nested1" aria-labelledby="ariaid-title10" id="s3_queries">

    <h2 class="title topictitle2" id="ariaid-title10">Running and Tuning Impala Queries for Data Stored in S3</h2>


    <div class="body conbody">
      <p class="p"> Once a table or partition is designated as residing in S3, the
          <code class="ph codeph">SELECT</code> statement transparently accesses the data
        files from the appropriate storage layer. </p>


      <ul class="ul">
        <li class="li">
          Queries against S3 data support all the same file formats as for HDFS data.
        </li>


        <li class="li">
          Tables can be unpartitioned or partitioned. For partitioned tables, either manually construct paths in S3
          corresponding to the HDFS directories representing partition key values, or use <code class="ph codeph">ALTER TABLE ...
          ADD PARTITION</code> to set up the appropriate paths in S3.
        </li>


        <li class="li">
          HDFS and HBase tables can be joined to S3 tables, or S3 tables can be joined with each other.
        </li>


        <li class="li"> Authorization to control access to databases, tables, or columns
          works the same whether the data is in HDFS or in S3. </li>

        <li class="li"> The Catalog Server (<span class="keyword cmdname">catalogd</span>) daemon caches
          metadata for both HDFS and S3 tables.</li>


        <li class="li">
          Queries against S3 tables are subject to the same kinds of admission control and resource management as
          HDFS tables.
        </li>


        <li class="li"> Metadata about S3 tables is stored in the same Metastore database
          as for HDFS tables. </li>


        <li class="li">
          You can set up views referring to S3 tables, the same as for HDFS tables.
        </li>


        <li class="li"> The <code class="ph codeph">COMPUTE STATS</code>, <code class="ph codeph">SHOW TABLE
            STATS</code>, and <code class="ph codeph">SHOW COLUMN STATS</code> statements
          work for S3 tables. </li>

      </ul>


    </div>


    <div class="topic concept nested2" aria-labelledby="ariaid-title11" id="s3_performance">

      <h3 class="title topictitle3" id="ariaid-title11">Understanding and Tuning Impala Query Performance for S3 Data</h3>

  

      <div class="body conbody">

        <p class="p">Here are techniques you can use to interpret explain plans and
          profiles for queries against S3 data, and tips to achieve the best
          performance possible for such queries. </p>


        <p class="p"> All else being equal, performance is expected to be lower for
          queries running against data in S3 rather than HDFS. The actual
          mechanics of the <code class="ph codeph">SELECT</code> statement are somewhat
          different when the data is in S3. Although the work is still
          distributed across the DataNodes of the cluster, Impala might
          parallelize the work for a distributed query differently for data on
          HDFS and S3.</p>

        <p class="p">S3 does not have the same block notion as HDFS, so Impala uses
          heuristics to determine how to split up large S3 files for processing
          in parallel. Because all hosts can access any S3 data file with equal
          efficiency, the distribution of work might be different than for HDFS
          data, where the data blocks are physically read using short-circuit
          local reads by hosts that contain the appropriate block replicas.
          Although the I/O to read the S3 data might be spread evenly across the
          hosts of the cluster, the fact that all data is initially retrieved
          across the network means that the overall query performance is likely
          to be lower for S3 data than for HDFS data. </p>

        <p class="p">Use the <code class="ph codeph">PARQUET_OBJECT_STORE_SPLIT_SIZE</code> query option
          to control the Parquet-specific split size. The default value is 256
          MB.</p>


        <p class="p"> When optimizing aspects of complex queries, such as the join order,
          Impala treats tables on HDFS and S3 the same way. Therefore, follow
          all the same tuning recommendations for S3 tables as for HDFS ones,
          such as using the <code class="ph codeph">COMPUTE STATS</code> statement to help
          Impala construct accurate estimates of row counts and cardinality. See
            <a class="xref" href="impala_performance.html#performance">Tuning Impala for Performance</a> for details. </p>


        <p class="p"> In query profile reports, the numbers for
            <code class="ph codeph">BytesReadLocal</code>,
            <code class="ph codeph">BytesReadShortCircuit</code>,
            <code class="ph codeph">BytesReadDataNodeCached</code>, and
            <code class="ph codeph">BytesReadRemoteUnexpected</code> are blank because those
          metrics come from HDFS. By definition, all the I/O for S3 tables
          involves remote reads. </p>


      </div>


    </div>


  </div>


  <div class="topic concept nested1" aria-labelledby="ariaid-title12" id="s3_restrictions">

    <h2 class="title topictitle2" id="ariaid-title12">Restrictions on Impala Support for S3</h2>


    <div class="body conbody">

      <p class="p">The following restrictions apply when using Impala with S3:</p>

      <ul class="ul">
        <li class="li"> Impala does not support the old <code class="ph codeph">s3://</code> block-based
          and <code class="ph codeph">s3n://</code> filesystem schemes, and it only supports
            <code class="ph codeph">s3a://</code>. </li>

        <li class="li">Although S3 is often used to store JSON-formatted data, the current
          Impala support for S3 does not include directly querying JSON data.
          For Impala queries, use data files in one of the file formats listed
          in <a class="xref" href="impala_file_formats.html#file_formats">How Impala Works with Hadoop File Formats</a>. If you have
          data in JSON format, you can prepare a flattened version of that data
          for querying by Impala as part of your ETL cycle. </li>

        <li class="li">You cannot use the <code class="ph codeph">ALTER TABLE ... SET CACHED</code>
          statement for tables or partitions that are located in S3. </li>

      </ul>


    </div>


  </div>



</body>
</html>