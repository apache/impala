<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

<meta name="copyright" content="(C) Copyright 2024" />
<meta name="DC.rights.owner" content="(C) Copyright 2024" />
<meta name="DC.Type" content="concept" />
<meta name="DC.Title" content="LOAD DATA Statement" />
<meta name="DC.Relation" scheme="URI" content="../topics/impala_langref_sql.html" />
<meta name="prodname" content="Impala" />
<meta name="prodname" content="Impala" />
<meta name="version" content="Impala 3.4.x" />
<meta name="version" content="Impala 3.4.x" />
<meta name="DC.Format" content="XHTML" />
<meta name="DC.Identifier" content="load_data" />
<link rel="stylesheet" type="text/css" href="../commonltr.css" />
<title>LOAD DATA Statement</title>
</head>
<body id="load_data">


  <h1 class="title topictitle1" id="ariaid-title1">LOAD DATA Statement</h1>

  
  

  <div class="body conbody">

    <p class="p"> The <code class="ph codeph">LOAD DATA</code> statement streamlines the ETL process for
      an internal Impala table by moving a data file or all the data files in a
      directory from an HDFS location into the Impala data directory for that
      table. </p>


    <p class="p">
        <strong class="ph b">Syntax:</strong>
      </p>


<pre class="pre codeblock"><code>LOAD DATA INPATH '<var class="keyword varname">hdfs_file_or_directory_path</var>' [OVERWRITE] INTO TABLE <var class="keyword varname">tablename</var>
  [PARTITION (<var class="keyword varname">partcol1</var>=<var class="keyword varname">val1</var>, <var class="keyword varname">partcol2</var>=<var class="keyword varname">val2</var> ...)]</code></pre>

    <p class="p">
      When the <code class="ph codeph">LOAD DATA</code> statement operates on a partitioned table,
      it always operates on one partition at a time. Specify the <code class="ph codeph">PARTITION</code> clauses
      and list all the partition key columns, with a constant value specified for each.
    </p>


    <p class="p">
        <strong class="ph b">Statement type:</strong> DML (but still affected by
        <a class="xref" href="../shared/../topics/impala_sync_ddl.html#sync_ddl">SYNC_DDL</a> query option)
      </p>


    <p class="p">
        <strong class="ph b">Usage notes:</strong>
      </p>


    <ul class="ul">
      <li class="li">
        The loaded data files are moved, not copied, into the Impala data directory.
      </li>


      <li class="li">
        You can specify the HDFS path of a single file to be moved, or the HDFS path of a directory to move all the
        files inside that directory. You cannot specify any sort of wildcard to take only some of the files from a
        directory. When loading a directory full of data files, keep all the data files at the top level, with no
        nested directories underneath.
      </li>


      <li class="li">
        Currently, the Impala <code class="ph codeph">LOAD DATA</code> statement only imports files from HDFS, not from the local
        filesystem. It does not support the <code class="ph codeph">LOCAL</code> keyword of the Hive <code class="ph codeph">LOAD DATA</code>
        statement. You must specify a path, not an <code class="ph codeph">hdfs://</code> URI.
      </li>


      <li class="li">
        In the interest of speed, only limited error checking is done. If the loaded files have the wrong file
        format, different columns than the destination table, or other kind of mismatch, Impala does not raise any
        error for the <code class="ph codeph">LOAD DATA</code> statement. Querying the table afterward could produce a runtime
        error or unexpected results. Currently, the only checking the <code class="ph codeph">LOAD DATA</code> statement does is
        to avoid mixing together uncompressed and LZO-compressed text files in the same table.
      </li>


      <li class="li">
        When you specify an HDFS directory name as the <code class="ph codeph">LOAD DATA</code> argument, any hidden files in
        that directory (files whose names start with a <code class="ph codeph">.</code>) are not moved to the Impala data
        directory.
      </li>


      <li class="li">
        The operation fails if the source directory contains any non-hidden directories.
        Prior to <span class="keyword">Impala 2.5</span> if the source directory contained any subdirectory, even a hidden one such as
        <span class="ph filepath">_impala_insert_staging</span>, the <code class="ph codeph">LOAD DATA</code> statement would fail.
        In <span class="keyword">Impala 2.5</span> and higher, <code class="ph codeph">LOAD DATA</code> ignores hidden subdirectories in the
        source directory, and only fails if any of the subdirectories are non-hidden.
      </li>


      <li class="li">
        The loaded data files retain their original names in the new location, unless a name conflicts with an
        existing data file, in which case the name of the new file is modified slightly to be unique. (The
        name-mangling is a slight difference from the Hive <code class="ph codeph">LOAD DATA</code> statement, which replaces
        identically named files.)
      </li>


      <li class="li">
        By providing an easy way to transport files from known locations in HDFS into the Impala data directory
        structure, the <code class="ph codeph">LOAD DATA</code> statement lets you avoid memorizing the locations and layout of
        HDFS directory tree containing the Impala databases and tables. (For a quick way to check the location of
        the data files for an Impala table, issue the statement <code class="ph codeph">DESCRIBE FORMATTED
        <var class="keyword varname">table_name</var></code>.)
      </li>


      <li class="li">
        The <code class="ph codeph">PARTITION</code> clause is especially convenient for ingesting new data for a partitioned
        table. As you receive new data for a time period, geographic region, or other division that corresponds to
        one or more partitioning columns, you can load that data straight into the appropriate Impala data
        directory, which might be nested several levels down if the table is partitioned by multiple columns. When
        the table is partitioned, you must specify constant values for all the partitioning columns.
      </li>

    </ul>


    <p class="p">
        <strong class="ph b">Complex type considerations:</strong>
      </p>


    <p class="p">
      Because Impala currently cannot create Parquet data files containing complex types
      (<code class="ph codeph">ARRAY</code>, <code class="ph codeph">STRUCT</code>, and <code class="ph codeph">MAP</code>), the
      <code class="ph codeph">LOAD DATA</code> statement is especially important when working with
      tables containing complex type columns. You create the Parquet data files outside
      Impala, then use either <code class="ph codeph">LOAD DATA</code>, an external table, or HDFS-level
      file operations followed by <code class="ph codeph">REFRESH</code> to associate the data files with
      the corresponding table.
      See <a class="xref" href="impala_complex_types.html#complex_types">Complex Types (Impala 2.3 or higher only)</a> for details about using complex types.
    </p>


    <p class="p">
        If you connect to different Impala nodes within an <span class="keyword cmdname">impala-shell</span>
        session for load-balancing purposes, you can enable the <code class="ph codeph">SYNC_DDL</code> query
        option to make each DDL statement wait before returning, until the new or changed
        metadata has been received by all the Impala nodes. See
        <a class="xref" href="../shared/../topics/impala_sync_ddl.html#sync_ddl">SYNC_DDL Query Option</a> for details.
      </p>


    <div class="note important"><span class="importanttitle">Important:</span> 
        After adding or replacing data in a table used in performance-critical queries, issue a
        <code class="ph codeph">COMPUTE STATS</code> statement to make sure all statistics are up-to-date.
        Consider updating statistics for a table after any <code class="ph codeph">INSERT</code>, <code class="ph codeph">LOAD
        DATA</code>, or <code class="ph codeph">CREATE TABLE AS SELECT</code> statement in Impala, or after
        loading data through Hive and doing a <code class="ph codeph">REFRESH
        <var class="keyword varname">table_name</var></code> in Impala. This technique is especially important
        for tables that are very large, used in join queries, or both.
      </div>


    <p class="p">
        <strong class="ph b">Examples:</strong>
      </p>


    <p class="p">
      First, we use a trivial Python script to write different numbers of strings (one per line) into files stored
      in the <code class="ph codeph">doc_demo</code> HDFS user account. (Substitute the path for your own HDFS user account when
      doing <span class="keyword cmdname">hdfs dfs</span> operations like these.)
    </p>


<pre class="pre codeblock"><code>$ random_strings.py 1000 | hdfs dfs -put - /user/doc_demo/thousand_strings.txt
$ random_strings.py 100 | hdfs dfs -put - /user/doc_demo/hundred_strings.txt
$ random_strings.py 10 | hdfs dfs -put - /user/doc_demo/ten_strings.txt</code></pre>

    <p class="p">
      Next, we create a table and load an initial set of data into it. Remember, unless you specify a
      <code class="ph codeph">STORED AS</code> clause, Impala tables default to <code class="ph codeph">TEXTFILE</code> format with Ctrl-A (hex
      01) as the field delimiter. This example uses a single-column table, so the delimiter is not significant. For
      large-scale ETL jobs, you would typically use binary format data files such as Parquet or Avro, and load them
      into Impala tables that use the corresponding file format.
    </p>


<pre class="pre codeblock"><code>[localhost:21000] &gt; create table t1 (s string);
[localhost:21000] &gt; load data inpath '/user/doc_demo/thousand_strings.txt' into table t1;
Query finished, fetching results ...
+----------------------------------------------------------+
| summary                                                  |
+----------------------------------------------------------+
| Loaded 1 file(s). Total files in destination location: 1 |
+----------------------------------------------------------+
Returned 1 row(s) in 0.61s
[kilo2-202-961.cs1cloud.internal:21000] &gt; select count(*) from t1;
Query finished, fetching results ...
+------+
| _c0  |
+------+
| 1000 |
+------+
Returned 1 row(s) in 0.67s
[localhost:21000] &gt; load data inpath '/user/doc_demo/thousand_strings.txt' into table t1;
ERROR: AnalysisException: INPATH location '/user/doc_demo/thousand_strings.txt' does not exist. </code></pre>

    <p class="p">
      As indicated by the message at the end of the previous example, the data file was moved from its original
      location. The following example illustrates how the data file was moved into the Impala data directory for
      the destination table, keeping its original filename:
    </p>


<pre class="pre codeblock"><code>$ hdfs dfs -ls /user/hive/warehouse/load_data_testing.db/t1
Found 1 items
-rw-r--r--   1 doc_demo doc_demo      13926 2013-06-26 15:40 /user/hive/warehouse/load_data_testing.db/t1/thousand_strings.txt</code></pre>

    <p class="p">
      The following example demonstrates the difference between the <code class="ph codeph">INTO TABLE</code> and
      <code class="ph codeph">OVERWRITE TABLE</code> clauses. The table already contains 1000 rows. After issuing the
      <code class="ph codeph">LOAD DATA</code> statement with the <code class="ph codeph">INTO TABLE</code> clause, the table contains 100 more
      rows, for a total of 1100. After issuing the <code class="ph codeph">LOAD DATA</code> statement with the <code class="ph codeph">OVERWRITE
      INTO TABLE</code> clause, the former contents are gone, and now the table only contains the 10 rows from
      the just-loaded data file.
    </p>


<pre class="pre codeblock"><code>[localhost:21000] &gt; load data inpath '/user/doc_demo/hundred_strings.txt' into table t1;
Query finished, fetching results ...
+----------------------------------------------------------+
| summary                                                  |
+----------------------------------------------------------+
| Loaded 1 file(s). Total files in destination location: 2 |
+----------------------------------------------------------+
Returned 1 row(s) in 0.24s
[localhost:21000] &gt; select count(*) from t1;
Query finished, fetching results ...
+------+
| _c0  |
+------+
| 1100 |
+------+
Returned 1 row(s) in 0.55s
[localhost:21000] &gt; load data inpath '/user/doc_demo/ten_strings.txt' overwrite into table t1;
Query finished, fetching results ...
+----------------------------------------------------------+
| summary                                                  |
+----------------------------------------------------------+
| Loaded 1 file(s). Total files in destination location: 1 |
+----------------------------------------------------------+
Returned 1 row(s) in 0.26s
[localhost:21000] &gt; select count(*) from t1;
Query finished, fetching results ...
+-----+
| _c0 |
+-----+
| 10  |
+-----+
Returned 1 row(s) in 0.62s</code></pre>

    <p class="p">
        <strong class="ph b">Amazon S3 considerations:</strong>
      </p>

    <p class="p"> In <span class="keyword">Impala 2.6</span> and higher, the Impala DML statements (<code class="ph codeph">INSERT</code>,
          <code class="ph codeph">LOAD DATA</code>, and <code class="ph codeph">CREATE TABLE AS
          SELECT</code>) can write data into a table or partition that resides
        in S3. The syntax of the DML statements is the same as for any other
        tables, because the S3 location for tables and partitions is specified
        by an <code class="ph codeph">s3a://</code> prefix in the <code class="ph codeph">LOCATION</code>
        attribute of <code class="ph codeph">CREATE TABLE</code> or <code class="ph codeph">ALTER
          TABLE</code> statements. If you bring data into S3 using the normal
        S3 transfer mechanisms instead of Impala DML statements, issue a
          <code class="ph codeph">REFRESH</code> statement for the table before using Impala
        to query the S3 data. </p>

    <p class="p"> Because of differences
        between S3 and traditional filesystems, DML operations for S3 tables can
        take longer than for tables on HDFS. For example, both the <code class="ph codeph">LOAD
          DATA</code> statement and the final stage of the
          <code class="ph codeph">INSERT</code> and <code class="ph codeph">CREATE TABLE AS SELECT</code>
        statements involve moving files from one directory to another. (In the
        case of <code class="ph codeph">INSERT</code> and <code class="ph codeph">CREATE TABLE AS
          SELECT</code>, the files are moved from a temporary staging
        directory to the final destination directory.) Because S3 does not
        support a <span class="q">"rename"</span> operation for existing objects, in these cases
        Impala actually copies the data files from one location to another and
        then removes the original files. In <span class="keyword">Impala 2.6</span>,
        the <code class="ph codeph">S3_SKIP_INSERT_STAGING</code> query option provides a way
        to speed up <code class="ph codeph">INSERT</code> statements for S3 tables and
        partitions, with the tradeoff that a problem during statement execution
        could leave data in an inconsistent state. It does not apply to
          <code class="ph codeph">INSERT OVERWRITE</code> or <code class="ph codeph">LOAD DATA</code>
        statements. See <a class="xref" href="../shared/../topics/impala_s3_skip_insert_staging.html#s3_skip_insert_staging">S3_SKIP_INSERT_STAGING Query Option</a> for details. </p>

    <p class="p">See <a class="xref" href="impala_s3.html#s3">Using Impala with Amazon S3 Object Store</a> for details about reading and writing S3 data with Impala.</p>


    <p class="p">
        <strong class="ph b">ADLS considerations:</strong>
      </p>

    <p class="p">
        In <span class="keyword">Impala 2.9</span> and higher, the Impala DML statements
        (<code class="ph codeph">INSERT</code>, <code class="ph codeph">LOAD DATA</code>, and <code class="ph codeph">CREATE TABLE AS
        SELECT</code>) can write data into a table or partition that resides in the Azure Data
        Lake Store (ADLS). ADLS Gen2 is supported in <span class="keyword">Impala 3.1</span> and higher.
      </p>
<p class="p">
        In the<code class="ph codeph">CREATE TABLE</code> or <code class="ph codeph">ALTER TABLE</code> statements, specify
        the ADLS location for tables and partitions with the <code class="ph codeph">adl://</code> prefix for
        ADLS Gen1 and <code class="ph codeph">abfs://</code> or <code class="ph codeph">abfss://</code> for ADLS Gen2 in the
        <code class="ph codeph">LOCATION</code> attribute.
      </p>
<p class="p">
        If you bring data into ADLS using the normal ADLS transfer mechanisms instead of Impala
        DML statements, issue a <code class="ph codeph">REFRESH</code> statement for the table before using
        Impala to query the ADLS data.
      </p>

    <p class="p">See <a class="xref" href="impala_adls.html#adls">Using Impala with the Azure Data Lake Store (ADLS)</a> for details about reading and writing ADLS data with Impala.</p>


    <p class="p">
        <strong class="ph b">Cancellation:</strong> Cannot be cancelled.
      </p>


    <p class="p">
        <strong class="ph b">HDFS permissions:</strong>
      </p>

    <p class="p">
      The user ID that the <span class="keyword cmdname">impalad</span> daemon runs under,
      typically the <code class="ph codeph">impala</code> user, must have read and write
      permissions for the files in the source directory, and write
      permission for the destination directory.
    </p>


    <p class="p">
        <strong class="ph b">Kudu considerations:</strong>
      </p>

    <p class="p">
        The <code class="ph codeph">LOAD DATA</code> statement cannot be used with Kudu tables.
      </p>


    <p class="p">
        <strong class="ph b">HBase considerations:</strong>
      </p>

    <p class="p">
        The <code class="ph codeph">LOAD DATA</code> statement cannot be used with HBase tables.
      </p>


    <p class="p">
        <strong class="ph b">Iceberg considerations:</strong>
      </p>

    <p class="p">
        See <a class="xref" href="../shared/../topics/impala_iceberg.html#iceberg_load">Loading data into Iceberg tables</a> for details about
        <code class="ph codeph">LOAD DATA</code> with Iceberg.
      </p>


    <p class="p">
        <strong class="ph b">Related information:</strong>
      </p>

    <p class="p">
      The <code class="ph codeph">LOAD DATA</code> statement is an alternative to the
      <code class="ph codeph"><a class="xref" href="impala_insert.html#insert">INSERT</a></code> statement.
      Use <code class="ph codeph">LOAD DATA</code>
      when you have the data files in HDFS but outside of any Impala table.
    </p>

    <p class="p">
      The <code class="ph codeph">LOAD DATA</code> statement is also an alternative
      to the <code class="ph codeph">CREATE EXTERNAL TABLE</code> statement. Use
      <code class="ph codeph">LOAD DATA</code> when it is appropriate to move the
      data files under Impala control rather than querying them
      from their original location. See <a class="xref" href="impala_tables.html#external_tables">External Tables</a>
      for information about working with external tables.
    </p>

  </div>

<div class="related-links">
<div class="familylinks">
<div class="parentlink"><strong>Parent topic:</strong> <a class="link" href="../topics/impala_langref_sql.html">Impala SQL Statements</a></div>
</div>
</div></body>
</html>