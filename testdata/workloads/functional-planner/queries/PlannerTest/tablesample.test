# Sample 10%
select * from functional.alltypes tablesample system(10) repeatable(1234)
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=36.00MB mem-reservation=4.03MB thread-reservation=2
PLAN-ROOT SINK
|  output exprs: functional.alltypes.id, functional.alltypes.bool_col, functional.alltypes.tinyint_col, functional.alltypes.smallint_col, functional.alltypes.int_col, functional.alltypes.bigint_col, functional.alltypes.float_col, functional.alltypes.double_col, functional.alltypes.date_string_col, functional.alltypes.string_col, functional.alltypes.timestamp_col, functional.alltypes.year, functional.alltypes.month
|  mem-estimate=4.00MB mem-reservation=4.00MB spill-buffer=2.00MB thread-reservation=0
|
00:SCAN HDFS [functional.alltypes]
   HDFS partitions=3/24 files=3 size=60.68KB
   stored statistics:
     table: rows=7.30K size=478.45KB
     partitions: 24/24 rows=7.30K
     columns: all
   extrapolated-rows=disabled max-scan-range-rows=2.45K
   mem-estimate=32.00MB mem-reservation=32.00KB thread-reservation=1
   tuple-ids=0 row-size=89B cardinality=730
   in pipelines: 00(GETNEXT)
====
# Sample 50%
select * from functional.alltypes tablesample system(50) repeatable(1234)
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=84.00MB mem-reservation=4.03MB thread-reservation=2
PLAN-ROOT SINK
|  output exprs: functional.alltypes.id, functional.alltypes.bool_col, functional.alltypes.tinyint_col, functional.alltypes.smallint_col, functional.alltypes.int_col, functional.alltypes.bigint_col, functional.alltypes.float_col, functional.alltypes.double_col, functional.alltypes.date_string_col, functional.alltypes.string_col, functional.alltypes.timestamp_col, functional.alltypes.year, functional.alltypes.month
|  mem-estimate=4.00MB mem-reservation=4.00MB spill-buffer=2.00MB thread-reservation=0
|
00:SCAN HDFS [functional.alltypes]
   HDFS partitions=12/24 files=12 size=239.26KB
   stored statistics:
     table: rows=7.30K size=478.45KB
     partitions: 24/24 rows=7.30K
     columns: all
   extrapolated-rows=disabled max-scan-range-rows=621
   mem-estimate=80.00MB mem-reservation=32.00KB thread-reservation=1
   tuple-ids=0 row-size=89B cardinality=3.65K
   in pipelines: 00(GETNEXT)
====
# Sampling and scan predicates. Scan predicates are evaluated after sampling and
# that is reflected in the cardinality.
select * from functional.alltypes tablesample system(50) repeatable(1234)
where id < 10
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=84.00MB mem-reservation=4.03MB thread-reservation=2
PLAN-ROOT SINK
|  output exprs: functional.alltypes.id, functional.alltypes.bool_col, functional.alltypes.tinyint_col, functional.alltypes.smallint_col, functional.alltypes.int_col, functional.alltypes.bigint_col, functional.alltypes.float_col, functional.alltypes.double_col, functional.alltypes.date_string_col, functional.alltypes.string_col, functional.alltypes.timestamp_col, functional.alltypes.year, functional.alltypes.month
|  mem-estimate=4.00MB mem-reservation=4.00MB spill-buffer=2.00MB thread-reservation=0
|
00:SCAN HDFS [functional.alltypes]
   HDFS partitions=12/24 files=12 size=239.26KB
   predicates: id < CAST(10 AS INT)
   stored statistics:
     table: rows=7.30K size=478.45KB
     partitions: 24/24 rows=7.30K
     columns: all
   extrapolated-rows=disabled max-scan-range-rows=621
   mem-estimate=80.00MB mem-reservation=32.00KB thread-reservation=1
   tuple-ids=0 row-size=89B cardinality=365
   in pipelines: 00(GETNEXT)
====
# Partition pruning + sampling. Partition pruning happens before sampling.
select * from functional.alltypes tablesample system(50) repeatable(1234)
where year = 2009
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=52.00MB mem-reservation=4.03MB thread-reservation=2
PLAN-ROOT SINK
|  output exprs: functional.alltypes.id, functional.alltypes.bool_col, functional.alltypes.tinyint_col, functional.alltypes.smallint_col, functional.alltypes.int_col, functional.alltypes.bigint_col, functional.alltypes.float_col, functional.alltypes.double_col, functional.alltypes.date_string_col, functional.alltypes.string_col, functional.alltypes.timestamp_col, functional.alltypes.year, functional.alltypes.month
|  mem-estimate=4.00MB mem-reservation=4.00MB spill-buffer=2.00MB thread-reservation=0
|
00:SCAN HDFS [functional.alltypes]
   partition predicates: `year` = CAST(2009 AS INT)
   HDFS partitions=6/24 files=6 size=119.70KB
   stored statistics:
     table: rows=7.30K size=478.45KB
     partitions: 12/12 rows=3.65K
     columns: all
   extrapolated-rows=disabled max-scan-range-rows=1.24K
   mem-estimate=48.00MB mem-reservation=32.00KB thread-reservation=1
   tuple-ids=0 row-size=89B cardinality=1.82K
   in pipelines: 00(GETNEXT)
====
# Edge case: sample 0%, no files should be selected
select * from functional.alltypes tablesample system(0)
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=4.00MB mem-reservation=4.00MB thread-reservation=1
PLAN-ROOT SINK
|  output exprs: functional.alltypes.id, functional.alltypes.bool_col, functional.alltypes.tinyint_col, functional.alltypes.smallint_col, functional.alltypes.int_col, functional.alltypes.bigint_col, functional.alltypes.float_col, functional.alltypes.double_col, functional.alltypes.date_string_col, functional.alltypes.string_col, functional.alltypes.timestamp_col, functional.alltypes.year, functional.alltypes.month
|  mem-estimate=4.00MB mem-reservation=4.00MB spill-buffer=2.00MB thread-reservation=0
|
00:SCAN HDFS [functional.alltypes]
   partitions=0/24 files=0 size=0B
   stored statistics:
     table: rows=7.30K size=478.45KB
     partitions: 24/24 rows=7.30K
     columns: all
   extrapolated-rows=disabled max-scan-range-rows=0
   mem-estimate=0B mem-reservation=0B thread-reservation=0
   tuple-ids=0 row-size=89B cardinality=0
   in pipelines: 00(GETNEXT)
====
# Edge case: sample 1%, at least one file should be selected
select * from functional.alltypes tablesample system(1) repeatable(1234)
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=36.00MB mem-reservation=4.03MB thread-reservation=2
PLAN-ROOT SINK
|  output exprs: functional.alltypes.id, functional.alltypes.bool_col, functional.alltypes.tinyint_col, functional.alltypes.smallint_col, functional.alltypes.int_col, functional.alltypes.bigint_col, functional.alltypes.float_col, functional.alltypes.double_col, functional.alltypes.date_string_col, functional.alltypes.string_col, functional.alltypes.timestamp_col, functional.alltypes.year, functional.alltypes.month
|  mem-estimate=4.00MB mem-reservation=4.00MB spill-buffer=2.00MB thread-reservation=0
|
00:SCAN HDFS [functional.alltypes]
   HDFS partitions=1/24 files=1 size=19.95KB
   stored statistics:
     table: rows=7.30K size=478.45KB
     partitions: 24/24 rows=7.30K
     columns: all
   extrapolated-rows=disabled max-scan-range-rows=7.30K
   mem-estimate=32.00MB mem-reservation=32.00KB thread-reservation=1
   tuple-ids=0 row-size=89B cardinality=73
   in pipelines: 00(GETNEXT)
====
# Edge case: sample 1% and prune partitions, at least one file should be selected
select * from functional.alltypes tablesample system(1) repeatable(1234)
where year = 2010
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=36.00MB mem-reservation=4.03MB thread-reservation=2
PLAN-ROOT SINK
|  output exprs: functional.alltypes.id, functional.alltypes.bool_col, functional.alltypes.tinyint_col, functional.alltypes.smallint_col, functional.alltypes.int_col, functional.alltypes.bigint_col, functional.alltypes.float_col, functional.alltypes.double_col, functional.alltypes.date_string_col, functional.alltypes.string_col, functional.alltypes.timestamp_col, functional.alltypes.year, functional.alltypes.month
|  mem-estimate=4.00MB mem-reservation=4.00MB spill-buffer=2.00MB thread-reservation=0
|
00:SCAN HDFS [functional.alltypes]
   partition predicates: `year` = CAST(2010 AS INT)
   HDFS partitions=1/24 files=1 size=20.36KB
   stored statistics:
     table: rows=7.30K size=478.45KB
     partitions: 12/12 rows=3.65K
     columns: all
   extrapolated-rows=disabled max-scan-range-rows=7.30K
   mem-estimate=32.00MB mem-reservation=32.00KB thread-reservation=1
   tuple-ids=0 row-size=89B cardinality=37
   in pipelines: 00(GETNEXT)
====
# Edge case: sample 100%, all files should be selected
select * from functional.alltypes tablesample system (100)
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=132.00MB mem-reservation=4.03MB thread-reservation=2
PLAN-ROOT SINK
|  output exprs: functional.alltypes.id, functional.alltypes.bool_col, functional.alltypes.tinyint_col, functional.alltypes.smallint_col, functional.alltypes.int_col, functional.alltypes.bigint_col, functional.alltypes.float_col, functional.alltypes.double_col, functional.alltypes.date_string_col, functional.alltypes.string_col, functional.alltypes.timestamp_col, functional.alltypes.year, functional.alltypes.month
|  mem-estimate=4.00MB mem-reservation=4.00MB spill-buffer=2.00MB thread-reservation=0
|
00:SCAN HDFS [functional.alltypes]
   HDFS partitions=24/24 files=24 size=478.45KB
   stored statistics:
     table: rows=7.30K size=478.45KB
     partitions: 24/24 rows=7.30K
     columns: all
   extrapolated-rows=disabled max-scan-range-rows=310
   mem-estimate=128.00MB mem-reservation=32.00KB thread-reservation=1
   tuple-ids=0 row-size=89B cardinality=7.30K
   in pipelines: 00(GETNEXT)
====
# Table that has no stats.
select id from functional_parquet.alltypes tablesample system(10) repeatable(1234)
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=20.00MB mem-reservation=4.02MB thread-reservation=2
PLAN-ROOT SINK
|  output exprs: id
|  mem-estimate=4.00MB mem-reservation=4.00MB spill-buffer=2.00MB thread-reservation=0
|
00:SCAN HDFS [functional_parquet.alltypes]
   HDFS partitions=3/24 files=3 size=23.83KB
   stored statistics:
     table: rows=unavailable size=unavailable
     partitions: 0/24 rows=12.01K
     columns: unavailable
   extrapolated-rows=disabled max-scan-range-rows=unavailable
   mem-estimate=16.00MB mem-reservation=16.00KB thread-reservation=1
   tuple-ids=0 row-size=4B cardinality=1.20K
   in pipelines: 00(GETNEXT)
====
# Sampling in a subquery.
select id from functional.alltypes t1 where exists (
  select id from functional.alltypessmall t2 tablesample system(10) repeatable(1234)
  where t1.id = t2.id)
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=162.94MB mem-reservation=6.97MB thread-reservation=3 runtime-filters-memory=1.00MB
PLAN-ROOT SINK
|  output exprs: id
|  mem-estimate=4.00MB mem-reservation=4.00MB spill-buffer=2.00MB thread-reservation=0
|
02:HASH JOIN [LEFT SEMI JOIN]
|  hash predicates: t1.id = t2.id
|  runtime filters: RF000[bloom] <- t2.id
|  mem-estimate=1.94MB mem-reservation=1.94MB spill-buffer=64.00KB thread-reservation=0
|  tuple-ids=0 row-size=4B cardinality=10
|  in pipelines: 00(GETNEXT), 01(OPEN)
|
|--01:SCAN HDFS [functional.alltypessmall t2]
|     HDFS partitions=1/4 files=1 size=1.57KB
|     stored statistics:
|       table: rows=100 size=6.32KB
|       partitions: 4/4 rows=100
|       columns: all
|     extrapolated-rows=disabled max-scan-range-rows=100
|     mem-estimate=32.00MB mem-reservation=8.00KB thread-reservation=1
|     tuple-ids=1 row-size=4B cardinality=10
|     in pipelines: 01(GETNEXT)
|
00:SCAN HDFS [functional.alltypes t1]
   HDFS partitions=24/24 files=24 size=478.45KB
   runtime filters: RF000[bloom] -> t1.id
   stored statistics:
     table: rows=7.30K size=478.45KB
     partitions: 24/24 rows=7.30K
     columns: all
   extrapolated-rows=disabled max-scan-range-rows=310
   mem-estimate=128.00MB mem-reservation=32.00KB thread-reservation=1
   tuple-ids=0 row-size=4B cardinality=7.30K
   in pipelines: 00(GETNEXT)
====
# Sampling in WITH-clause view.
with t as (select * from functional.alltypes tablesample system(10) repeatable(1234))
select id from t
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=36.00MB mem-reservation=4.03MB thread-reservation=2
PLAN-ROOT SINK
|  output exprs: functional.alltypes.id
|  mem-estimate=4.00MB mem-reservation=4.00MB spill-buffer=2.00MB thread-reservation=0
|
00:SCAN HDFS [functional.alltypes]
   HDFS partitions=3/24 files=3 size=60.68KB
   stored statistics:
     table: rows=7.30K size=478.45KB
     partitions: 24/24 rows=7.30K
     columns: all
   extrapolated-rows=disabled max-scan-range-rows=2.45K
   mem-estimate=32.00MB mem-reservation=32.00KB thread-reservation=1
   tuple-ids=0 row-size=4B cardinality=730
   in pipelines: 00(GETNEXT)
====
# Sampling Iceberg tables.
select * from functional_parquet.iceberg_non_partitioned tablesample system(10) repeatable(1234)
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=68.00MB mem-reservation=4.03MB thread-reservation=2
PLAN-ROOT SINK
|  output exprs: functional_parquet.iceberg_non_partitioned.id, functional_parquet.iceberg_non_partitioned.user, functional_parquet.iceberg_non_partitioned.action, functional_parquet.iceberg_non_partitioned.event_time
|  mem-estimate=4.00MB mem-reservation=4.00MB spill-buffer=2.00MB thread-reservation=0
|
00:SCAN HDFS [functional_parquet.iceberg_non_partitioned]
   HDFS partitions=1/1 files=3 size=3.41KB
   Iceberg snapshot id: 93996984692289973
   stored statistics:
     table: rows=20 size=22.90KB
     columns: unavailable
   extrapolated-rows=disabled max-scan-range-rows=6
   mem-estimate=64.00MB mem-reservation=32.00KB thread-reservation=1
   tuple-ids=0 row-size=44B cardinality=3
   in pipelines: 00(GETNEXT)
====
# Sampling Iceberg tables. Count(*) is optimized.
select count(*) from functional_parquet.iceberg_non_partitioned tablesample system(10) repeatable(1234)
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=1.02MB mem-reservation=8.00KB thread-reservation=2
PLAN-ROOT SINK
|  output exprs: count(*)
|  mem-estimate=0B mem-reservation=0B thread-reservation=0
|
01:AGGREGATE [FINALIZE]
|  output: sum_init_zero(functional_parquet.iceberg_non_partitioned.stats: num_rows)
|  mem-estimate=16.00KB mem-reservation=0B spill-buffer=2.00MB thread-reservation=0
|  tuple-ids=1 row-size=8B cardinality=1
|  in pipelines: 01(GETNEXT), 00(OPEN)
|
00:SCAN HDFS [functional_parquet.iceberg_non_partitioned]
   HDFS partitions=1/1 files=3 size=3.41KB
   Iceberg snapshot id: 93996984692289973
   stored statistics:
     table: rows=20 size=22.90KB
     columns: all
   extrapolated-rows=disabled max-scan-range-rows=6
   mem-estimate=1.00MB mem-reservation=8.00KB thread-reservation=1
   tuple-ids=0 row-size=8B cardinality=20
   in pipelines: 00(GETNEXT)
====
# Sampling partitioned Iceberg tables.
select * from functional_parquet.iceberg_partitioned tablesample system(50) repeatable(1234)
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=68.00MB mem-reservation=4.03MB thread-reservation=2
PLAN-ROOT SINK
|  output exprs: functional_parquet.iceberg_partitioned.id, functional_parquet.iceberg_partitioned.user, functional_parquet.iceberg_partitioned.action, functional_parquet.iceberg_partitioned.event_time
|  mem-estimate=4.00MB mem-reservation=4.00MB spill-buffer=2.00MB thread-reservation=0
|
00:SCAN HDFS [functional_parquet.iceberg_partitioned]
   HDFS partitions=1/1 files=10 size=11.46KB
   Iceberg snapshot id: 8270633197658268308
   stored statistics:
     table: rows=20 size=22.90KB
     columns: unavailable
   extrapolated-rows=disabled max-scan-range-rows=2
   mem-estimate=64.00MB mem-reservation=32.00KB thread-reservation=1
   tuple-ids=0 row-size=44B cardinality=10
   in pipelines: 00(GETNEXT)
====
# Sampling Iceberg tables with predicates. Predicate pushdown to Iceberg happens
# before sampling (similarly to static partition pruning).
select * from functional_parquet.iceberg_partitioned tablesample system(50) repeatable(1234)
where action = 'click' and id > 0
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=68.00MB mem-reservation=4.03MB thread-reservation=2
PLAN-ROOT SINK
|  output exprs: functional_parquet.iceberg_partitioned.id, functional_parquet.iceberg_partitioned.user, functional_parquet.iceberg_partitioned.action, functional_parquet.iceberg_partitioned.event_time
|  mem-estimate=4.00MB mem-reservation=4.00MB spill-buffer=2.00MB thread-reservation=0
|
00:SCAN HDFS [functional_parquet.iceberg_partitioned]
   HDFS partitions=1/1 files=4 size=4.57KB
   predicates: id > CAST(0 AS INT)
   Iceberg snapshot id: 8270633197658268308
   skipped Iceberg predicates: action = 'click'
   stored statistics:
     table: rows=20 size=22.90KB
     columns: unavailable
   extrapolated-rows=disabled max-scan-range-rows=5
   parquet statistics predicates: id > CAST(0 AS INT)
   parquet dictionary predicates: id > CAST(0 AS INT)
   mem-estimate=64.00MB mem-reservation=32.00KB thread-reservation=1
   tuple-ids=0 row-size=44B cardinality=1
   in pipelines: 00(GETNEXT)
====
# Sampling Iceberg V2 tables. Delete files are not sampled, only the data files. So we
# don't return rows that are deleted.
select * from functional_parquet.iceberg_v2_positional_not_all_data_files_have_delete_files
tablesample system(10) repeatable(1234)
---- PLAN
F00:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
|  Per-Host Resources: mem-estimate=100.00MB mem-reservation=4.05MB thread-reservation=3
PLAN-ROOT SINK
|  output exprs: functional_parquet.iceberg_v2_positional_not_all_data_files_have_delete_files.i, functional_parquet.iceberg_v2_positional_not_all_data_files_have_delete_files.s
|  mem-estimate=4.00MB mem-reservation=4.00MB spill-buffer=2.00MB thread-reservation=0
|
04:UNION
|  pass-through-operands: all
|  mem-estimate=0B mem-reservation=0B thread-reservation=0
|  tuple-ids=0 row-size=36B cardinality=2
|  in pipelines: 03(GETNEXT), 00(GETNEXT)
|
|--02:DELETE EVENTS ICEBERG DELETE [ICEBERG DELETE JOIN]
|  |  equality predicates: functional_parquet.iceberg_v2_positional_not_all_data_files_have_delete_files.file__position = functional_parquet.iceberg_v2_positional_not_all_data_files_have_delete_files-position-delete.pos, functional_parquet.iceberg_v2_positional_not_all_data_files_have_delete_files.input__file__name = functional_parquet.iceberg_v2_positional_not_all_data_files_have_delete_files-position-delete.file_path
|  |  mem-estimate=566B mem-reservation=566B thread-reservation=0
|  |  tuple-ids=0 row-size=36B cardinality=1
|  |  in pipelines: 00(GETNEXT), 01(OPEN)
|  |
|  |--01:SCAN HDFS [functional_parquet.iceberg_v2_positional_not_all_data_files_have_delete_files-POSITION-DELETE-01 functional_parquet.iceberg_v2_positional_not_all_data_files_have_delete_files-position-delete]
|  |     HDFS partitions=1/1 files=2 size=5.33KB
|  |     Iceberg snapshot id: 1497619269847778439
|  |     stored statistics:
|  |       table: rows=4 size=5.33KB
|  |       columns: all
|  |     extrapolated-rows=disabled max-scan-range-rows=2
|  |     mem-estimate=32.00MB mem-reservation=16.00KB thread-reservation=1
|  |     tuple-ids=1 row-size=267B cardinality=4
|  |     in pipelines: 01(GETNEXT)
|  |
|  00:SCAN HDFS [functional_parquet.iceberg_v2_positional_not_all_data_files_have_delete_files]
|     HDFS partitions=1/1 files=1 size=625B
|     Iceberg snapshot id: 1497619269847778439
|     stored statistics:
|       table: rows=10 size=7.77KB
|       columns missing stats: i, s
|     extrapolated-rows=disabled max-scan-range-rows=10
|     mem-estimate=64.00MB mem-reservation=32.00KB thread-reservation=1
|     tuple-ids=0 row-size=36B cardinality=3
|     in pipelines: 00(GETNEXT)
|
03:SCAN HDFS [functional_parquet.iceberg_v2_positional_not_all_data_files_have_delete_files]
   HDFS partitions=1/1 files=1 size=620B
   Iceberg snapshot id: 1497619269847778439
   stored statistics:
     table: rows=10 size=7.77KB
     columns missing stats: i, s
   extrapolated-rows=disabled max-scan-range-rows=10
   mem-estimate=64.00MB mem-reservation=32.00KB thread-reservation=1
   tuple-ids=0 row-size=36B cardinality=1
   in pipelines: 03(GETNEXT)
====
